{
  
    
        "post0": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages&#182; . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages&#182; . In this post, we will cover special features that fastpages provides has for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter&#182; . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding&#182; . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions . Interactive Charts With Altair&#182; . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables&#182; . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Images w/Captions&#182; . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards&#182; . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos&#182; . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts&#182; . Typing &gt; Warning: There will be no second warning! will render this: Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: Note: A doc link to an example website: fast.ai should also work fine. . More Examples&#182; . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts&#182; . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps&#182; . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://www.dataguasu.com/blog/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About&#182; . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter&#182; . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts&#182; . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair&#182; . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown&#182; . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips&#182; . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips&#182; . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables&#182; . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images&#182; . Local Images&#182; . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images&#182; . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs&#182; . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions&#182; . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements&#182; . Tweetcards&#182; . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos&#182; . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts&#182; . Typing &gt; Warning: There will be no second warning! will render this: Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: Note: A doc link to an example website: fast.ai should also work fine. . Footnotes&#182; . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://www.dataguasu.com/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Example Markdown Post",
            "content": "Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . You can include alert boxes …and… . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.dataguasu.com/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Reclamos ciudadanos en Twitter",
            "content": "Introducci&#243;n&#182; . Desde hace ya unos años, las redes sociales se volvieron el medio de comunicación preferido de los ciudadanos para realizar reclamos relacionados a la provisión de servicios públicos e infraestructura (electricidad, agua potable, recolección de basura, reportes de baches, etc.) . Si bien esto produjo un avance importante en la comunicación ciudadanía-autoridades, el exceso y la velocidad de generación de la información impide tener un análisis certero de los reclamos como para reaccionar de manera eficaz, entender la causa raíz y prevenir futuros eventos. . Entender esto nos ayudaría por ejemplo a responder las siguientes preguntas: . ¿Cuáles son los reclamos más frecuentes realizados por los ciudadanos? | ¿Cómo varía la intensidad de estos reclamos en el tiempo? | ¿Cuál es el sentimiento de las publicaciones realizadas por los ciudadanos hacia las autoridades? | . Con el objetivo de lograr un entendimiento mas profundo que nos permita responder a estas y otras preguntas, utilizo los tweets o publicaciones realizadas en Twitter donde se mencionan a la Municipalidad de Asunción (@AsuncionMuni) y al Intendente (@FerreiroMario1). . An&#225;lisis Exploratorio&#182; . Librerias&#182; . En primer lugar, importamos todas las librerias necesarias para el análisis. En este caso, decidí utilizar Pandas para la manipulación de los datos con los conocidos dataframes, seaborn y matplotlib para visualizaciones, NLTK para tokenización, en conjunto con sklearn para implementar técnicas como TF-IDF que son útiles para extraer las combinaciones de palabras más relevantes y como entrada para otras técnicas como LDA, que detallo más adelante. . import datetime, re, spacy, nltk import calendar from time import time from nltk.corpus import stopwords from nltk.tokenize.toktok import ToktokTokenizer from nltk.tokenize import TweetTokenizer from nltk import ngrams import string import pandas as pd import numpy as np from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator from PIL import Image from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer from sklearn.decomposition import LatentDirichletAllocation from sklearn.pipeline import Pipeline from collections import Counter import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline . Carga de tweets a pandas dataframe&#182; . Los tweets se encuentran contenidos en un archivo cuya estructura se detalla en el apartado de datos en Kaggle. Además de cargar los tweets a un dataframe de pandas, derivo otros atributos que serán útiles más adelante. Extraer algunos tweets de ejemplo puede ser útil para entender los tipos de datos en cada columna. . df_tweets_muni = pd.read_csv(&quot;../input/tweets-municipalidad-asuncion/tweets_municipalidad.csv&quot;) df_tweets_muni[&#39;created_at&#39;] = pd.to_datetime(df_tweets_muni[&#39;created_at&#39;]) df_tweets_muni[&#39;date&#39;] = pd.to_datetime(df_tweets_muni[&#39;created_at&#39;].dt.date) df_tweets_muni[&#39;month&#39;] = df_tweets_muni[&#39;date&#39;].dt.month df_tweets_muni[&#39;year&#39;] = df_tweets_muni[&#39;date&#39;].dt.year df_tweets_muni[&#39;tweet&#39;] = df_tweets_muni[&#39;tweet&#39;].astype(str) df_tweets_muni[&#39;year_month&#39;] = df_tweets_muni[&#39;date&#39;].dt.to_period(&#39;M&#39;) . df_tweets_muni.head(5) . created_at tweet username date month year year_month . 0 2017-10-30 23:57:03+00:00 | exelente saludos desde nycity | renevera2013 | 2017-10-30 | 10 | 2017 | 2017-10 | . 1 2017-10-30 23:50:51+00:00 | Ahora mismo 5 autos &quot;agarran&quot; el carril derech... | Superman74Cacer | 2017-10-30 | 10 | 2017 | 2017-10 | . 2 2017-10-30 23:50:01+00:00 | Siempre la misma cosa frente a CASACOR... | Superman74Cacer | 2017-10-30 | 10 | 2017 | 2017-10 | . 3 2017-10-30 23:47:24+00:00 | ESAS PAREDES....SE TIENE QUE LLAMAR A LOS CAND... | aweissman1950 | 2017-10-30 | 10 | 2017 | 2017-10 | . 4 2017-10-30 23:46:34+00:00 | Hacen años que ahí no se puede girar a la izqu... | aristidesag | 2017-10-30 | 10 | 2017 | 2017-10 | . Deduplicaci&#243;n y filtrado&#182; . En las siguientes celdas, elimino los tweets duplicados que puedieron aparecer al realizar la extracción o en caso que usuarios hayan publicado el mismo tweet varias veces. . Luego, identifico los usuarios con más cantidad de tweets y excluyo aquellas cuentas relacionadas a la Municipalidad, medios de comunicación u otros entes públicos de tal manera a considerar únicamente las publicaciones de los ciudadanos. . # Borrar tweets duplicados (nos quedamos con el primero) df_tweets_muni = df_tweets_muni.drop_duplicates(subset=[&#39;tweet&#39;], keep=&#39;first&#39;) . # Usuarios con mas tweets df_tweets_username = df_tweets_muni.groupby(&#39;username&#39;).count().reset_index() df_tweets_username.sort_values(by=&#39;tweet&#39;, ascending=False)[[&#39;username&#39;,&#39;tweet&#39;]].head(10) . username tweet . 26216 pmtasuncion1 | 11679 | . 1413 AsuncionMuni | 4881 | . 1409 AsuDsu | 3273 | . 3830 El_Rafa_PY | 1984 | . 247 ABCCardinal | 1207 | . 7146 LaUnionAM | 1182 | . 194 780AM | 1071 | . 28362 teclitamovil | 841 | . 1408 AsuDgrrd | 731 | . 15991 chrispresspy | 722 | . # Excluir cuentas de la municipalidad o de medios de comunicacion filtro = ~(df_tweets_muni[&#39;username&#39;].isin([&#39;AsuncionMuni&#39;,&#39;pmtasuncion1&#39;,&#39;AsuDsu&#39;,&#39;ABCCardinal&#39;, &#39;LaUnionAM&#39;, &#39;780AM&#39;, &#39;AsuDgrrd&#39;, &#39;Universo970py&#39;, &#39;1000_am&#39;, &#39;EssapSA&#39;, &#39;Ferreiromario1&#39;, &#39;AbastoAsu&#39;, &#39;ANDEOficial&#39;, &#39;mopcparaguay&#39;])) # Crear copia de dataframe con filtro aplicado df_tweets_muni_filtro = df_tweets_muni[filtro].copy() . Limpieza&#182; . Los tweets publicados pueden tener información poco relevante para análisis textuales como por ejemplo URLs, emails, referencias a imagenes o simbolos. Además, existen palabras conocidas como stopwords que se repiten frecuentemente y no aportan al entendimiento de la conversación (el, la, y, con, para, mi, etc.). NLTK es una librería de NLP que incluye stopwords en diferentes idiomas, incluyendo español. . # Cargar stopwords en español stopwords_es = stopwords.words(&#39;spanish&#39;) &quot;&quot;&quot; Excluir menciones, emails, URLs y simbolos &quot;&quot;&quot; def clean_tweet(tweet): # Convertir a minusculas tweet = tweet.lower() # Excluir menciones o emails tweet = re.sub(r&#39; w*@( w+ .* w+ .* w+)&#39;,&#39; &#39;, tweet) # Excluir simbolos tweet = tweet.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) # Excluir URLs tweet = re.sub(r&#39;(?:www .|https?)[^ s]+&#39;, &#39; &#39;, tweet, flags=re.MULTILINE) # Borrar espacios tweet = tweet.strip() # Considerar solo valores alfa numericos tweet_alfa = re.compile(&quot;^(?![0-9]*$)[a-zA-Z0-9]+$&quot;) # Eliminar stopwords y palabras con longitud &lt;= 2 tokens = tweet.split() text = [token for token in tokens if token not in stopwords_es and len(token)&gt;2 and tweet_alfa.match(token)] return &#39; &#39;.join(text) # Aplicar filtro a tweets df_tweets_muni_filtro[&#39;tweet_cleaned&#39;] = df_tweets_muni_filtro[&#39;tweet&#39;].apply(clean_tweet) . Bigramas frecuentes por a&#241;o&#182; . Los bigramas son combinaciones de dos palabras que pueden dar una mejor idea de los temas de conversación. En este caso, me interesa conocer los bigramas que más se repiten y para ellos aplico técnicas de tokenización que separan las palabras del texto y cada par se convierte en una fila. También se puede modificar el tamaño del ngram para formar unigramas, trigramas, etc. El bigrama es una opción intermedia que permite tener algo más de contexto pero tiene suficientes ocurrencias para que sea significativa la muestra (mientras mayor sea el ngram, menor el número de ocurrencias). . # Tamano del ngram ngram = 2 tokenizer = TweetTokenizer() # Tokenizar y aplicar ngram df_tweets_muni_filtro[&#39;tokenize&#39;] = df_tweets_muni_filtro[&#39;tweet_cleaned&#39;] .apply(tokenizer.tokenize) df_tweets_muni_filtro[&#39;ngram&#39;] = df_tweets_muni_filtro[&#39;tokenize&#39;] .apply(lambda x: list(ngrams(x, ngram))) # Una fila por ngram df_tweets_muni_exploded = df_tweets_muni_filtro .explode(&#39;ngram&#39;)[[&#39;date&#39;,&#39;year&#39;,&#39;ngram&#39;]] . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: generator &#39;ngrams&#39; raised StopIteration if __name__ == &#39;__main__&#39;: . # Agrupar por cantidad de ocurrencias df_tweets_muni_exploded_grouped = df_tweets_muni_exploded .groupby([&#39;year&#39;, &#39;ngram&#39;]) .agg({&#39;date&#39;:&#39;count&#39;}) .reset_index() .sort_values(by=[&#39;year&#39;, &#39;date&#39;], ascending=False) .rename(columns={&#39;date&#39;:&#39;count&#39;}) # Top 10 por YYYY df_tweets_muni_top_year = df_tweets_muni_exploded_grouped .drop_duplicates(subset=[&#39;count&#39;,&#39;year&#39;]) .groupby([&#39;year&#39;]) .head(10) . La siguiente gráfica muestra las ocurrencias de cada bigrama por año. Se pueden identificar por ejemplo bigramas que aparecen en multiples años . import seaborn as sns ax = sns.catplot(x=&quot;count&quot;,y=&quot;ngram&quot;, col=&quot;year&quot;, data=df_tweets_muni_top_year, kind=&quot;bar&quot;, height=5, aspect=.7); ax.set(xlabel=&quot;Frequencia&quot;, ylabel=&quot;Bigrama&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f949202b978&gt; . Wordcloud&#182; . Las nubes de palabras o word clouds permiten visualizar las palabras más frecuentes de un texto utilizando el tamaño para representar la frecuencia o importancia. En este caso, las palabras se extraen de los tweets filtrados (cerca de 200.000). . Para volverlo un poco más divertido y patriota, utilizo un fondo con nuestra bandera pero se puede adaptar a cualquier tipo de imagen. . # Generación de un wordcloud paraguayo texto_tweets = &#39; &#39;.join(df_tweets_muni_filtro[&#39;tweet_cleaned&#39;]) mask = np.array(Image.open(&quot;../input/paraguay-flag/paraguay_flag_.png&quot;)) wordcloud_py = WordCloud(background_color=&quot;white&quot;, mode=&quot;RGBA&quot;, max_words=1000, mask=mask).generate(texto_tweets) # Utilización de colores de la imagen image_colors = ImageColorGenerator(mask) plt.figure(figsize=[12,8]) plt.imshow(wordcloud_py.recolor(color_func=image_colors), interpolation=&quot;bilinear&quot;) plt.axis(&quot;off&quot;) . (-0.5, 594.5, 223.5, -0.5) . Usuarios &#250;nicos&#182; . La cantidad de usuarios únicos que publican tweets nos puede ayudar a identificar situaciones que provocaron mayores picos de participación en el tiempo. Si nos fijamos la gráfica de abajo, se observan ciertos picos en el 2019, tanto en Mayo como en Julio. ¿Qué se mencionaba con frecuencia en estas fechas? . df_unique_users = df_tweets_muni_filtro.groupby([&#39;date&#39;])[&#39;username&#39;].nunique() . fig = plt.figure(figsize=(15,5)) sns.lineplot(x=&#39;date&#39;, y=&#39;username&#39;, data=df_unique_users.reset_index()) plt.title(&#39;Usuarios Unicos por Dia&#39;) plt.ylabel(&#39;Usuarios&#39;) plt.xlabel(&#39;Fecha&#39;) . /opt/conda/lib/python3.6/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters. To register the converters: &gt;&gt;&gt; from pandas.plotting import register_matplotlib_converters &gt;&gt;&gt; register_matplotlib_converters() warnings.warn(msg, FutureWarning) . Text(0.5, 0, &#39;Fecha&#39;) . df_unique_users.reset_index().sort_values(&#39;username&#39;, ascending=False).head(5) . date username . 854 2019-05-11 | 1098 | . 853 2019-05-10 | 1083 | . 926 2019-07-23 | 973 | . 698 2018-12-03 | 861 | . 709 2018-12-14 | 782 | . &quot;gente puerca&quot;, &quot;basura calle&quot;, &quot;tira basura&quot;.. estos bigramas dan para pensar que en estas fechas pudo haber llovido con mucha frequencia, lo que pudo haber ocasionado raudales que a su vez movieron las basuras de un lado para otro. . fig = plt.figure(figsize=(10,5)) df_top_unique_days = df_tweets_muni_exploded[df_tweets_muni_exploded[&#39;date&#39;].isin([&#39;2019-05-10&#39;,&#39;2019-05-11&#39;])] .groupby(&#39;ngram&#39;) .count() .reset_index() .sort_values(&#39;date&#39;, ascending=False) .head(10) sns.barplot(x=&#39;date&#39;, y=&#39;ngram&#39;, data=df_top_unique_days, orient=&#39;h&#39;) plt.title(&#39;Top bigramas para 10-11/05/2019&#39;) plt.xlabel(&#39;Menciones&#39;) plt.ylabel(&#39;Bigrama&#39;) . Text(0, 0.5, &#39;Bigrama&#39;) . An&#225;lisis por eventos&#182; . Con lo analizado hasta ahora, tenemos una leve idea de los temas de conversación. Además, existen otros temas que pueden ser interesantes y no se encuentran a simple vista. Para entender un poco mejor como estos temas se mencionan en el tiempo, utilizo expresiones regulares que de acuerdo a ciertos patrones de búsqueda identifiquen tweets relacionados a los temas que nos interesan. En este caso, elegí los siguientes temas que me parecieron los más reclamados por la ciudadanía: basura, raudales, baches y dengue. Es importante tener en cuenta que un mismo tweet puede pertenecer a mas de una categoria porque de hecho puede existir correlacion entre varios de estos temas. . # Filtrar tweets por categorias de acuerdo a palabras claves def get_df_from_criteria(df, criteria): df = df[df[&#39;tweet_cleaned&#39;] .str .contains(criteria, flags=re.IGNORECASE, regex=True)] .groupby([&#39;date&#39;,&#39;year&#39;,&#39;month&#39;], as_index=False) .agg([&#39;count&#39;])[&#39;created_at&#39;].reset_index().rename(columns={&#39;count&#39;:&#39;tweets&#39;}) return df.copy() filtro_baches = r&#39; bbache| bvache| bcrater&#39; baches_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_baches) filtro_basura = r&#39; bbasura| brecicla| bdesecho| btoxico| bvertedero| bescombro| bsucio| basco&#39; basura_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_basura) filtro_inundado = r&#39; binunda| blluvia| bllueve| braudal| bdesagu*&#39; inundado_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_inundado) filtro_dengue = r&#39; bdengue| bmosquito| baedes| bcriadero| bminga| bfumiga&#39; dengue_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_dengue) . fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) sns.lineplot(x=&#39;date&#39;, y=&#39;tweets&#39;, data=baches_x_dia_df[baches_x_dia_df[&#39;year&#39;].isin([2017,2018,2019])], ax=ax) sns.lineplot(x=&#39;date&#39;, y=&#39;tweets&#39;, data=basura_x_dia_df[basura_x_dia_df[&#39;year&#39;].isin([2017,2018,2019])], ax=ax) sns.lineplot(x=&#39;date&#39;, y=&#39;tweets&#39;, data=inundado_x_dia_df[inundado_x_dia_df[&#39;year&#39;].isin([2017,2018,2019])], ax=ax) sns.lineplot(x=&#39;date&#39;, y=&#39;tweets&#39;, data=dengue_x_dia_df[dengue_x_dia_df[&#39;year&#39;].isin([2017,2018,2019])], ax=ax) plt.title(&#39;Cantidad de Menciones por Dia&#39;) plt.ylabel(&#39;Menciones&#39;) plt.xlabel(&#39;Fecha&#39;) ax.legend([&#39;Baches&#39;, &#39;Basura&#39;, &#39;Raudales&#39;, &#39;Dengue&#39;], loc=&#39;upper left&#39;, prop={&#39;size&#39;: 12}) . &lt;matplotlib.legend.Legend at 0x7f9491c8c828&gt; . En Mayo vemos picos de menciones a raudales que coinciden con los picos de usuarios unicos que vimos mas arriba. También se observa un pico de conversaciones relacionadas al dengue en Marzo de 2018 que coincide con uno de los brotes mas importantes en los últimos años. . Para facilitar comparaciones interanuales, en las próximas gráficas podemos ver para cada tema como cambian las menciones en el tiempo. En el caso de dengue por ejemplo, se observa un aumento importante en Marzo de 2018. Por otro lado, en las menciones de raudales se ve un crecimiento notorio en Mayo de 2019 en comparación con años anteriores. . # Agrupar por mes dengue_x_mes_df = dengue_x_dia_df.groupby([&#39;year&#39;,&#39;month&#39;]) .sum().reset_index()[[&#39;month&#39;,&#39;year&#39;,&#39;tweets&#39;]] # Pivotear dengue_x_mes_pivot = dengue_x_mes_df .pivot(index=&#39;month&#39;,columns=&#39;year&#39;, values=&#39;tweets&#39;) dengue_x_mes_pivot.plot(kind=&#39;bar&#39;, figsize=(15, 5), color=[&#39;lightgray&#39;, &#39;gray&#39;, &#39;black&#39;], rot=0) plt.title(&quot;Comparaciones interanuales de menciones de Dengue&quot;) plt.xlabel(&quot;Mes&quot;, labelpad=16) plt.ylabel(&quot;Menciones (Dengue)&quot;, labelpad=16) . Text(0, 0.5, &#39;Menciones (Dengue)&#39;) . # Agrupar por mes basura_x_mes_df = basura_x_dia_df.groupby([&#39;year&#39;,&#39;month&#39;]) .sum().reset_index()[[&#39;month&#39;,&#39;year&#39;,&#39;tweets&#39;]] # Pivotear basura_x_mes_pivot = basura_x_mes_df .pivot(index=&#39;month&#39;,columns=&#39;year&#39;, values=&#39;tweets&#39;) basura_x_mes_pivot.plot(kind=&#39;bar&#39;, figsize=(15, 5), color=[&#39;lightgray&#39;, &#39;gray&#39;, &#39;black&#39;], rot=0) plt.title(&quot;Comparaciones interanuales de menciones de Basura&quot;) plt.xlabel(&quot;Mes&quot;, labelpad=16) plt.ylabel(&quot;Menciones (Basura)&quot;, labelpad=16) . Text(0, 0.5, &#39;Menciones (Basura)&#39;) . # Agrupar por mes inundado_x_mes_df = inundado_x_dia_df.groupby([&#39;year&#39;,&#39;month&#39;]) .sum().reset_index()[[&#39;month&#39;,&#39;year&#39;,&#39;tweets&#39;]] # Pivotear inundado_x_mes_pivot = inundado_x_mes_df .pivot(index=&#39;month&#39;,columns=&#39;year&#39;, values=&#39;tweets&#39;) inundado_x_mes_pivot.plot(kind=&#39;bar&#39;, figsize=(15, 5), color=[&#39;lightgray&#39;, &#39;gray&#39;, &#39;black&#39;], rot=0) plt.title(&quot;Comparaciones interanuales de menciones de Raudales&quot;) plt.xlabel(&quot;Mes&quot;, labelpad=16) plt.ylabel(&quot;Menciones (Raudales)&quot;, labelpad=16) . Text(0, 0.5, &#39;Menciones (Raudales)&#39;) . Modelado de Topicos con LDA&#182; . Para casos como este, dónde tenemos una buena idea de los temas de conversación en los tweets, las expresiones regulares pueden ser suficientes. Sin embargo, existen otros casos dónde se necesitan de técnicas más avanzadas para identificar temas que pueden estar escondidos, o latentes. . Existen diferentes técnicas de identificación de temas o tópicos pero una de las más utilizadas es Latent Dirichlet Allocation o LDA. Se trata de una técnica que genera un modelo probabilístico que asume que cada tema es una combinación de palabras y que cada documento (o tweet en este caso) es una combinación de temas con diferentes probabilidades. . En las celdas de abajo, creo un Pipeline que primero aplica una técnica conocida como TF-IDF que calcula la frecuencia de palabras en los tweets, y calcula un score para cada palabra dando menos importancia a aquellas que aparecen con demasiada frecuencia y son poco relevantes. En el siguiente paso del pipeline se entrena el modelo y transforma el dataframe original. En este caso, elijo clasificar los temas en 5 diferentes categorias pero el número puede variar de acuerdo al caso. Existen formas de medir la calidad del modelo, calculando lo que se denomina perplexity pero no lo voy a utilizar en este caso. . Otro punto importante es que el entrenamiento del modelo puede tomar mucho tiempo y por esto es clave definir ciertos parametros que limiten la busqueda y la cantidad de iteraciones. . # Cantidad de topicos n_topics = 5 # Pipeline con pasos a ejectuar text_pipeline = Pipeline([ (&#39;tfidf&#39;, TfidfVectorizer(ngram_range=(2,3), min_df=100, max_df=0.85)), (&#39;lda&#39;, LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method=&#39;online&#39;)) ]) t0 = time() # Entrenar y transformar modelo lda_model = text_pipeline.fit_transform(df_tweets_muni_filtro[&#39;tweet_cleaned&#39;]) print(time() - t0) . 129.6550097465515 . # Modelos resultantes del pipeline tfidf = text_pipeline.steps[0][1] lda = text_pipeline.steps[1][1] vocabulario = tfidf.get_feature_names() . top_topics = 5 topic_dict = {} topic_scores = [] # Para cada topico, buscamos el top 5 de acuerdo a los scores calculados por el modelo for topic_idx, topic in enumerate(lda.components_): topic_dict[str(topic_idx)] = &quot;,&quot;.join([vocabulario[i] for i in topic.argsort()[:-top_topics - 1:-1]]) topic_scores.append([topic[i] for i in topic.argsort()[:-top_topics - 1:-1]]) df_topics_lda = pd.DataFrame(topic_dict, index=[&#39;bigrams&#39;]) df_topics_lda = df_topics_lda.T.reset_index() # Cargamos las palabras claves por topico en un dataframe df_topics_names = pd.DataFrame(df_topics_lda.bigrams.str.split(&#39;,&#39;).tolist(), index=df_topics_lda.index) .stack() .reset_index() .drop([&#39;level_1&#39;], axis=1) .rename(columns={0:&#39;bigrams&#39;, &#39;level_0&#39;:&#39;topic&#39;}) # Cargamos scores por topicos en un dataframe df_topics_scores = pd.DataFrame(topic_scores) .stack() .reset_index(drop=True) # Concatenamos palabras claves con scores correspondientes df_topics = pd.concat([df_topics_names, df_topics_scores], axis=1) .rename(columns={0:&#39;score&#39;}) . Finalmente se observan los bigramas mas frecuentes para cada una de las categorias. No se identifican categorias especificas y esto puede deberse a que no existe una diferenciacion muy clara entre los temas de conversacion. También se pueden obtener mejores resultados realizando validaciones adicionales y excluyendo otras combinaciones poco relevantes. . fig = plt.figure(figsize=(12,10)) for i in range(1,n_topics+1): plt.subplot(3,2,i,frameon=True) sns.barplot(&#39;score&#39;, &#39;bigrams&#39;, data=df_topics[df_topics[&#39;topic&#39;]==i-1], orient=&#39;h&#39;) plt.title(&quot;Topico {}&quot;.format(i)) plt.xlabel(&#39;&#39;) plt.ylabel(&#39;&#39;) plt.tight_layout() . Otros an&#225;lisis que se podrian realizar con estos datos:&#182; . Análisis de sentimiento | Correlación entre temas y visualización con scattertext | Series temporales para predecir eventos (como brotes de dengue) | .",
            "url": "https://www.dataguasu.com/blog/twitter/nlp/eda/2019/11/18/analisis_tweets_municipalidad.html",
            "relUrl": "/twitter/nlp/eda/2019/11/18/analisis_tweets_municipalidad.html",
            "date": " • Nov 18, 2019"
        }
        
    
  
    
        ,"post4": {
            "title": "Analysis of ATP Tennis competitions from 1968 to 2019",
            "content": "The dataset prepared by Jeff Sackmann contains information on every single match played since 1968. It includes details on every match’s date, location, tournament type, surface, winner and loser, games/sets played, duration and additional statistics such as players’ rankings, players’ age and height, aces, double faults, break points faced and saved, service points among other helpful stats for both players of the match. . The objective of this analysis is to come up with insights regarding the tournaments including correlation between variables (e.g. aces and players’ height) as well as the evolution of the players over time, looking at the different variables and using different types of visualizations as tools to facilitate interpretation of the data and communication of the results. . import matplotlib.pyplot as plt import pandas as pd import seaborn as sns import numpy as np import random import os import glob sns.set_style(&#39;white&#39;) %matplotlib inline plt.rcParams.update({&#39;font.size&#39;: 14}) . Data loading, transforming and initial exploration&#182; . We start off with basic data exploration using pairplots and histograms to better understand the distribution of the key variables of our dataset based on multiple dimensions. Data conversion is also done to convert string values to float for a few columns that contain numerical values. . # Load data tennis_df = pd.read_csv(&quot;../input/ATP.csv&quot;, dtype=str) . # Convert numerical values to float numeric_columns = [&#39;w_ace&#39;, &#39;l_ace&#39;, &#39;w_df&#39;, &#39;l_df&#39;, &#39;w_bpSaved&#39;, &#39;l_bpSaved&#39;, &#39;winner_rank&#39;, &#39;loser_rank&#39;, &#39;winner_age&#39;, &#39;loser_age&#39;, &#39;winner_ht&#39;, &#39;loser_ht&#39;, &#39;w_svpt&#39;, &#39;l_svpt&#39;, &#39;minutes&#39;] tennis_df[numeric_columns] = tennis_df[numeric_columns].astype(float) . # Create new columns storing year and year/month attributes tennis_df[&#39;tourney_yearmonth&#39;] = tennis_df.tourney_date.astype(str).str[:6] tennis_df[&#39;tourney_year&#39;] = tennis_df.tourney_date.astype(str).str[:4] tennis_df[&#39;tourney_year&#39;] = tennis_df[&#39;tourney_year&#39;].astype(int) . # Use pairplot to draw multiple variables at once df_pp = tennis_df[tennis_df[&#39;tourney_year&#39;].between(2010,2019)&amp;tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;,&#39;G&#39;])&amp;tennis_df[&#39;round&#39;].isin([&#39;F&#39;])][[&#39;minutes&#39;,&#39;loser_rank&#39;,&#39;winner_rank&#39;,&#39;tourney_level&#39;]].dropna() sns.pairplot(df_pp, hue=&#39;tourney_level&#39;,diag_kind=&quot;kde&quot;, plot_kws=dict(s=50, edgecolor=&quot;b&quot;, linewidth=1), diag_kws=dict(shade=True)) . &lt;seaborn.axisgrid.PairGrid at 0x7f6157c46748&gt; . dimensions = [&#39;winner_rank&#39;,&#39;loser_rank&#39;,&#39;winner_age&#39;,&#39;loser_age&#39;,&#39;winner_ht&#39;,&#39;loser_ht&#39;,&#39;w_svpt&#39;,&#39;l_svpt&#39;] plt.figure(1, figsize=(20,12)) for i in range(1,9): plt.subplot(2,4,i) tennis_df[dimensions[i-1]].plot(kind=&#39;hist&#39;, title=dimensions[i-1]) . Distribution of aces by surface type&#182; . What is the distribution of aces by the different type of surfaces? . tennis_df_h = tennis_df[tennis_df[&#39;tourney_level&#39;].astype(str).isin([&#39;G&#39;,&#39;M&#39;])].copy() tennis_df_h[&#39;w_ace&#39;] = tennis_df_h[&#39;w_ace&#39;].astype(float) g = sns.boxplot(x=&quot;surface&quot;, y=&quot;w_ace&quot;, data=tennis_df_h) g.set(xlabel=&#39;Surface&#39;, ylabel=&#39;Aces&#39;) . [Text(0, 0.5, &#39;Aces&#39;), Text(0.5, 0, &#39;Surface&#39;)] . We can see in this boxplot that grass and hard courts have a higher incidence of aces as compared to clay courts . Evolution of top countries represented by their players&#182; . fig, ax = plt.subplots(figsize=(15, 6)) s = tennis_df[(tennis_df[&#39;tourney_level&#39;] == &#39;G&#39;) &amp; (tennis_df[&#39;winner_ioc&#39;].isin([&#39;ARG&#39;]))].groupby([&#39;tourney_year&#39;,&#39;winner_ioc&#39;]).agg(&#39;count&#39;) plt.plot(s[&#39;tourney_id&#39;].index.get_level_values(0), s[&#39;tourney_id&#39;].values, color=&#39;blue&#39;, linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=2, label=&#39;Argentina&#39;) s = tennis_df[(tennis_df[&#39;tourney_level&#39;] == &#39;G&#39;) &amp; (tennis_df[&#39;winner_ioc&#39;].isin([&#39;ESP&#39;]))].groupby([&#39;tourney_year&#39;,&#39;winner_ioc&#39;]).agg(&#39;count&#39;) plt.plot(s[&#39;tourney_id&#39;].index.get_level_values(0), s[&#39;tourney_id&#39;].values, color=&#39;green&#39;, linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=2, label=&#39;Spain&#39;) s = tennis_df[(tennis_df[&#39;tourney_level&#39;] == &#39;G&#39;) &amp; (tennis_df[&#39;winner_ioc&#39;].isin([&#39;SUI&#39;]))].groupby([&#39;tourney_year&#39;,&#39;winner_ioc&#39;]).agg(&#39;count&#39;) ax.plot(s[&#39;tourney_id&#39;].index.get_level_values(0), s[&#39;tourney_id&#39;].values, color=&#39;red&#39;, linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=2, label=&#39;Switzarland&#39;) s = tennis_df[(tennis_df[&#39;tourney_level&#39;] == &#39;G&#39;) &amp; (tennis_df[&#39;winner_ioc&#39;].isin([&#39;USA&#39;]))].groupby([&#39;tourney_year&#39;,&#39;winner_ioc&#39;]).agg(&#39;count&#39;) ax.plot(s[&#39;tourney_id&#39;].index.get_level_values(0), s[&#39;tourney_id&#39;].values, color=&#39;magenta&#39;, linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=2, label=&#39;USA&#39;) s = tennis_df[(tennis_df[&#39;tourney_level&#39;] == &#39;G&#39;) &amp; (tennis_df[&#39;winner_ioc&#39;].isin([&#39;SRB&#39;]))].groupby([&#39;tourney_year&#39;,&#39;winner_ioc&#39;]).agg(&#39;count&#39;) ax.plot(s[&#39;tourney_id&#39;].index.get_level_values(0), s[&#39;tourney_id&#39;].values, color=&#39;grey&#39;, linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;blue&#39;, markersize=2, label=&#39;Serbia&#39;) #labels = s.index.get_level_values(0) ax.set_ylabel(&#39;Number of Wins&#39;) ax.set_title(&#39;Number of GS Wins per Country&#39;) #ax.set_xticklabels(labels) ax.legend() . &lt;matplotlib.legend.Legend at 0x7f6157d1ae48&gt; . There is an increase in the number of wins from players from Argentina since the early 2000s . There is an increase in the number of wins in the cases of Spain, Switzerland and Serbia (New top players —&gt; Nadal, Federer and Djokovic) . Top most aces, double faults and break points saved&#182; . sw = tennis_df.groupby([&#39;winner_name&#39;]).agg({&#39;w_ace&#39;:&#39;sum&#39;}).fillna(0).sort_values([&#39;w_ace&#39;], ascending=False) sl = tennis_df.groupby([&#39;loser_name&#39;]).agg({&#39;l_ace&#39;:&#39;sum&#39;}).fillna(0).sort_values([&#39;l_ace&#39;], ascending=False) dfs = [sw,sl] r = pd.concat(dfs, sort=False).reset_index().fillna(0) r[&#39;aces&#39;] = r[&#39;l_ace&#39;]+r[&#39;w_ace&#39;] final = r.groupby(&#39;index&#39;).agg({&#39;aces&#39;:&#39;sum&#39;}).sort_values(&#39;aces&#39;,ascending=False).head(10) final = final.reset_index() final.columns = [&#39;Player&#39;,&#39;Aces&#39;] final = final.sort_values(&#39;Aces&#39;,ascending=True) final.plot(&#39;Player&#39;,&#39;Aces&#39;, kind=&#39;barh&#39;, title=&#39;Players with most Aces&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6157c947f0&gt; . sw = tennis_df.groupby([&#39;winner_name&#39;]).agg({&#39;w_df&#39;:&#39;sum&#39;}).fillna(0).sort_values([&#39;w_df&#39;], ascending=False) sl = tennis_df.groupby([&#39;loser_name&#39;]).agg({&#39;l_df&#39;:&#39;sum&#39;}).fillna(0).sort_values([&#39;l_df&#39;], ascending=False) dfs = [sw,sl] r = pd.concat(dfs, sort=False).reset_index().fillna(0) r[&#39;dfs&#39;] = r[&#39;l_df&#39;]+r[&#39;w_df&#39;] final = r.groupby(&#39;index&#39;).agg({&#39;dfs&#39;:&#39;sum&#39;}).sort_values(&#39;dfs&#39;,ascending=False).head(10) final = final.reset_index() final.columns = [&#39;Player&#39;,&#39;DoubleFaults&#39;] final = final.sort_values(&#39;DoubleFaults&#39;,ascending=True) final.plot(&#39;Player&#39;,&#39;DoubleFaults&#39;, kind=&#39;barh&#39;, title=&#39;Players with most Double Faults&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6156840630&gt; . sw = tennis_df.groupby([&#39;winner_name&#39;]).agg({&#39;w_bpSaved&#39;:&#39;sum&#39;}).fillna(0).sort_values([&#39;w_bpSaved&#39;], ascending=False) sl = tennis_df.groupby([&#39;loser_name&#39;]).agg({&#39;l_bpSaved&#39;:&#39;sum&#39;}).fillna(0).sort_values([&#39;l_bpSaved&#39;], ascending=False) dfs = [sw,sl] r = pd.concat(dfs, sort=False).reset_index().fillna(0) r[&#39;dfs&#39;] = r[&#39;l_bpSaved&#39;]+r[&#39;w_bpSaved&#39;] final = r.groupby(&#39;index&#39;).agg({&#39;dfs&#39;:&#39;sum&#39;}).sort_values(&#39;dfs&#39;,ascending=False).head(10) final = final.reset_index() final.columns = [&#39;Player&#39;,&#39;BreakPointsSaved&#39;] final = final.sort_values(&#39;BreakPointsSaved&#39;,ascending=True) final.plot(&#39;Player&#39;,&#39;BreakPointsSaved&#39;, kind=&#39;barh&#39;, title=&#39;Players with most BP saved&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f61580b2b00&gt; . Evolution of top players in the last 20 years&#182; . pldf_1 = tennis_df[(tennis_df[&#39;winner_name&#39;] == &#39;Roger Federer&#39;)].groupby([&#39;tourney_year&#39;,&#39;tourney_level&#39;], as_index=False).agg([&#39;count&#39;]) pldf_2 = pldf_1[&#39;tourney_id&#39;].reset_index() pldf_2 = pldf_2.sort_values(by=[&#39;tourney_year&#39;]) fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) ax.set_prop_cycle(plt.cycler(&#39;color&#39;, plt.cm.jet(np.linspace(0, 1, 5)))) plt.title(&#39;Roger Federer - Total Wins by Tournament Type by Year&#39;) plt.ylabel(&#39;Number of Wins&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2, linewidth=3) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;F&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;F&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;D&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;D&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.legend([&#39;Grand_Slams&#39;, &#39;Masters&#39;, &#39;Tour_Finals&#39;, &#39;Davis_Cup&#39;, &#39;ATP&#39;], loc=&#39;upper right&#39;, prop={&#39;size&#39;: 10}) . &lt;matplotlib.legend.Legend at 0x7f615813cd68&gt; . pldf_1 = tennis_df[(tennis_df[&#39;winner_name&#39;] == &#39;Rafael Nadal&#39;)].groupby([&#39;tourney_year&#39;,&#39;tourney_level&#39;], as_index=False).agg([&#39;count&#39;]) pldf_2 = pldf_1[&#39;tourney_id&#39;].reset_index() fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) ax.set_prop_cycle(plt.cycler(&#39;color&#39;, plt.cm.jet(np.linspace(0, 1, 5)))) plt.title(&#39;Rafael Nadal - Total Wins by Tournament Type by Year&#39;) plt.ylabel(&#39;Number of Wins&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2, linewidth=3) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;F&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;F&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;D&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;D&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.legend([&#39;Grand_Slams&#39;, &#39;Masters&#39;, &#39;Tour_Finals&#39;, &#39;Davis_Cup&#39;, &#39;ATP&#39;], loc=&#39;upper right&#39;) . &lt;matplotlib.legend.Legend at 0x7f6157f5a860&gt; . pldf_1 = tennis_df[(tennis_df[&#39;winner_name&#39;] == &#39;Novak Djokovic&#39;)].groupby([&#39;tourney_year&#39;,&#39;tourney_level&#39;], as_index=False).agg([&#39;count&#39;]) pldf_2 = pldf_1[&#39;tourney_id&#39;].reset_index() fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) ax.set_prop_cycle(plt.cycler(&#39;color&#39;, plt.cm.jet(np.linspace(0, 1, 5)))) plt.title(&#39;Novak Djokovic - Total Wins by Tournament Type by Year&#39;) plt.ylabel(&#39;Number of Wins&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2, linewidth=3) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;F&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;F&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;D&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;D&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.plot(pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;tourney_year&#39;], pldf_2[pldf_2[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;count&#39;], linestyle=&#39;dashed&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=2) plt.legend([&#39;Grand_Slams&#39;, &#39;Masters&#39;, &#39;Tour_Finals&#39;, &#39;Davis_Cup&#39;, &#39;ATP&#39;], loc=&#39;upper left&#39;) . &lt;matplotlib.legend.Legend at 0x7f61580f2c18&gt; . Dominance: Unique number of players that won most important tournaments&#182; . # Unique number of tournament winner per year from 2000 to 2016 (show dominance of top players) s = tennis_df[(tennis_df[&#39;round&#39;]==&#39;F&#39;)&amp;(tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;,&#39;G&#39;]))].groupby([&#39;tourney_year&#39;]).agg({&#39;winner_name&#39;:&#39;nunique&#39;}) t= s.reset_index() t.columns=[&#39;Year&#39;,&#39;Unique_Winners&#39;] t.plot(&#39;Year&#39;, &#39;Unique_Winners&#39;, title=&#39;Unique # of Players that Won GS and Masters Finals&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f61580a3ef0&gt; . # Unique number of tournament winner per year from 2000 to 2016 (show dominance of top players) s = tennis_df[(tennis_df[&#39;round&#39;]==&#39;F&#39;)&amp;(tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;,&#39;G&#39;]))&amp;(tennis_df[&#39;tourney_year&#39;].between(2000,2003))].agg({&#39;winner_name&#39;:&#39;nunique&#39;}) t = tennis_df[(tennis_df[&#39;round&#39;]==&#39;F&#39;)&amp;(tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;,&#39;G&#39;]))&amp;(tennis_df[&#39;tourney_year&#39;].between(2004,2007))].agg({&#39;winner_name&#39;:&#39;nunique&#39;}) u = tennis_df[(tennis_df[&#39;round&#39;]==&#39;F&#39;)&amp;(tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;,&#39;G&#39;]))&amp;(tennis_df[&#39;tourney_year&#39;].between(2008,2011))].agg({&#39;winner_name&#39;:&#39;nunique&#39;}) v = tennis_df[(tennis_df[&#39;round&#39;]==&#39;F&#39;)&amp;(tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;,&#39;G&#39;]))&amp;(tennis_df[&#39;tourney_year&#39;].between(2012,2015))].agg({&#39;winner_name&#39;:&#39;nunique&#39;}) w = tennis_df[(tennis_df[&#39;round&#39;]==&#39;F&#39;)&amp;(tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;,&#39;G&#39;]))&amp;(tennis_df[&#39;tourney_year&#39;].between(2016,2019))].agg({&#39;winner_name&#39;:&#39;nunique&#39;}) s[&#39;2000-2003&#39;] = s[&#39;winner_name&#39;] s=s.drop(&#39;winner_name&#39;) t[&#39;2004-2007&#39;] = t[&#39;winner_name&#39;] t=t.drop(&#39;winner_name&#39;) u[&#39;2008-2011&#39;] = u[&#39;winner_name&#39;] u=u.drop(&#39;winner_name&#39;) v[&#39;2012-2015&#39;] = v[&#39;winner_name&#39;] v=v.drop(&#39;winner_name&#39;) w[&#39;2016-2019&#39;] = w[&#39;winner_name&#39;] w=w.drop(&#39;winner_name&#39;) dfl = [s,t,u,v,w] dfs = pd.concat(dfl) x=pd.DataFrame(dfs, columns=[&#39;Unique_Count&#39;]).reset_index() x.columns=[&#39;Year_Range&#39;,&#39;Unique_Winners&#39;] x.plot(&#39;Year_Range&#39;, &#39;Unique_Winners&#39;, kind=&#39;bar&#39;, title=&#39;Unique # of Players that Won GS and Masters Finals&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6158057e48&gt; . Relationship between Aces and Height of players&#182; . g1 = sns.lmplot(x=&#39;w_ace&#39;, y=&#39;winner_ht&#39;, hue=&#39;surface&#39;, fit_reg=True, data=tennis_df[(tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;]))&amp;(tennis_df[&#39;tourney_year&#39;]==2000)]) g1.fig.suptitle(&#39;Aces vs. Height in Masters - Year 2000&#39;) g1.set(xlabel=&#39;Aces&#39;, ylabel=&#39;Height&#39;) g2 = sns.lmplot(x=&#39;w_ace&#39;, y=&#39;winner_ht&#39;, hue=&#39;surface&#39;, fit_reg=True, data=tennis_df[(tennis_df[&#39;tourney_level&#39;].isin([&#39;M&#39;]))&amp;(tennis_df[&#39;tourney_year&#39;]==2018)]) g2.fig.suptitle(&#39;Aces vs. Height in Masters - Year 2018&#39;) g2.set(xlabel=&#39;Aces&#39;, ylabel=&#39;Height&#39;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f6157fe2da0&gt; . When comparing 2018 to 2000 scatter plots we notice that in recent years there is a higher relation of aces to the height of the players The number of taller players with more aces have also increased . Effectiveness of top players by surface type&#182; . pw = tennis_df[(tennis_df[&#39;winner_name&#39;] == &#39;Roger Federer&#39;)].groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg([&#39;count&#39;]) pww = pw[&#39;tourney_id&#39;].reset_index() pl = tennis_df[(tennis_df[&#39;loser_name&#39;] == &#39;Roger Federer&#39;)].groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg([&#39;count&#39;]) pll = pl[&#39;tourney_id&#39;].reset_index() pww.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;wins&#39;] pll.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;loses&#39;] dfs = (pww,pll) dfs_concat = pd.concat(dfs, sort=False) dfs_final = dfs_concat.fillna(0).groupby([&#39;tourney_year&#39;,&#39;surface&#39;]).agg({&#39;wins&#39;:&#39;sum&#39;,&#39;loses&#39;:&#39;sum&#39;}).reset_index() dfs_final[&#39;r_eff&#39;] = np.where(dfs_final[&#39;loses&#39;]&gt;0, dfs_final[&#39;wins&#39;]/(dfs_final[&#39;wins&#39;]+dfs_final[&#39;loses&#39;]), 1) dfs_final[&#39;tourney_year&#39;] = dfs_final[&#39;tourney_year&#39;].astype(int) #sns.swarmplot(x=&quot;tourney_year&quot;, y=&quot;r_eff&quot;, hue=&quot;surface&quot;, data=dfs_final); g = sns.lmplot(x=&#39;tourney_year&#39;, y=&#39;r_eff&#39;, hue=&#39;surface&#39;, fit_reg=False, data=dfs_final, palette=&#39;viridis&#39;, hue_order=[&#39;Hard&#39;,&#39;Carpet&#39;,&#39;Grass&#39;,&#39;Clay&#39;]) #sns.barplot(x=&quot;tourney_year&quot;, y=&quot;r_eff&quot;, hue=&quot;surface&quot;, data=dfs_final) g.fig.suptitle(&#39;Roger Federer - Effectiveness&#39;) g.set(xlabel=&#39;Year&#39;, ylabel=&#39;Effectiveness&#39;) g.set(ylim=(-0.1,1.2)) . &lt;seaborn.axisgrid.FacetGrid at 0x7f6156793f98&gt; . pw = tennis_df[(tennis_df[&#39;winner_name&#39;] == &#39;Rafael Nadal&#39;)].groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg([&#39;count&#39;]) pww = pw[&#39;tourney_id&#39;].reset_index() pl = tennis_df[(tennis_df[&#39;loser_name&#39;] == &#39;Rafael Nadal&#39;)].groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg([&#39;count&#39;]) pll = pl[&#39;tourney_id&#39;].reset_index() pww.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;wins&#39;] pll.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;loses&#39;] dfs = (pww,pll) dfs_concat = pd.concat(dfs, sort=False) dfs_final = dfs_concat.fillna(0).groupby([&#39;tourney_year&#39;,&#39;surface&#39;]).agg({&#39;wins&#39;:&#39;sum&#39;,&#39;loses&#39;:&#39;sum&#39;}).reset_index() dfs_final[&#39;r_eff&#39;] = np.where(dfs_final[&#39;loses&#39;]&gt;0, dfs_final[&#39;wins&#39;]/(dfs_final[&#39;wins&#39;]+dfs_final[&#39;loses&#39;]), 1) dfs_final[&#39;tourney_year&#39;] = dfs_final[&#39;tourney_year&#39;].astype(int) g = sns.lmplot(x=&#39;tourney_year&#39;, y=&#39;r_eff&#39;, hue=&#39;surface&#39;, fit_reg=False, data=dfs_final, palette=&#39;viridis&#39;, hue_order=[&#39;Hard&#39;,&#39;Carpet&#39;,&#39;Grass&#39;,&#39;Clay&#39;]) g.fig.suptitle(&#39;Rafael Nadal - Effectiveness&#39;) g.set(xlabel=&#39;Year&#39;, ylabel=&#39;Effectiveness&#39;) g.set(ylim=(-0.1,1.2)) . &lt;seaborn.axisgrid.FacetGrid at 0x7f6156797ba8&gt; . pw = tennis_df[(tennis_df[&#39;winner_name&#39;] == &#39;Novak Djokovic&#39;)].groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg([&#39;count&#39;]) pww = pw[&#39;tourney_id&#39;].reset_index() pl = tennis_df[(tennis_df[&#39;loser_name&#39;] == &#39;Novak Djokovic&#39;)].groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg([&#39;count&#39;]) pll = pl[&#39;tourney_id&#39;].reset_index() pww.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;wins&#39;] pll.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;loses&#39;] dfs = (pww,pll) dfs_concat = pd.concat(dfs, sort=False) dfs_final = dfs_concat.fillna(0).groupby([&#39;tourney_year&#39;,&#39;surface&#39;]).agg({&#39;wins&#39;:&#39;sum&#39;,&#39;loses&#39;:&#39;sum&#39;}).reset_index() dfs_final[&#39;r_eff&#39;] = np.where(dfs_final[&#39;loses&#39;]&gt;0, dfs_final[&#39;wins&#39;]/(dfs_final[&#39;wins&#39;]+dfs_final[&#39;loses&#39;]), 1) dfs_final[&#39;tourney_year&#39;] = dfs_final[&#39;tourney_year&#39;].astype(int) g = sns.lmplot(x=&#39;tourney_year&#39;, y=&#39;r_eff&#39;, hue=&#39;surface&#39;, fit_reg=False, data=dfs_final, palette=&#39;viridis&#39;, hue_order=[&#39;Hard&#39;,&#39;Carpet&#39;,&#39;Grass&#39;,&#39;Clay&#39;]) g.fig.suptitle(&#39;Novak Djokovic - Effectiveness&#39;) g.set(xlabel=&#39;Year&#39;, ylabel=&#39;Effectiveness&#39;) g.set(ylim=(-0.1,1.2)) . &lt;seaborn.axisgrid.FacetGrid at 0x7f6156760320&gt; . pw = tennis_df[(tennis_df[&#39;winner_name&#39;] == &#39;Stanislas Wawrinka&#39;)].groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg([&#39;count&#39;]) pww = pw[&#39;tourney_id&#39;].reset_index() pl = tennis_df[(tennis_df[&#39;loser_name&#39;] == &#39;Stanislas Wawrinka&#39;)].groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg([&#39;count&#39;]) pll = pl[&#39;tourney_id&#39;].reset_index() pww.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;wins&#39;] pll.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;loses&#39;] dfs = (pww,pll) dfs_concat = pd.concat(dfs, sort=False) dfs_final = dfs_concat.fillna(0).groupby([&#39;tourney_year&#39;,&#39;surface&#39;]).agg({&#39;wins&#39;:&#39;sum&#39;,&#39;loses&#39;:&#39;sum&#39;}).reset_index() dfs_final[&#39;r_eff&#39;] = np.where(dfs_final[&#39;loses&#39;]&gt;0, dfs_final[&#39;wins&#39;]/(dfs_final[&#39;wins&#39;]+dfs_final[&#39;loses&#39;]), 1) dfs_final[&#39;tourney_year&#39;] = dfs_final[&#39;tourney_year&#39;].astype(int) g = sns.lmplot(x=&#39;tourney_year&#39;, y=&#39;r_eff&#39;, hue=&#39;surface&#39;, fit_reg=False, data=dfs_final, palette=&#39;viridis&#39;, hue_order=[&#39;Hard&#39;,&#39;Carpet&#39;,&#39;Grass&#39;,&#39;Clay&#39;]) g.fig.suptitle(&#39;Stanislas Wawrinka - Effectiveness&#39;) g.set(xlabel=&#39;Year&#39;, ylabel=&#39;Effectiveness&#39;) g.set(ylim=(-0.1,1.2)) . &lt;seaborn.axisgrid.FacetGrid at 0x7f615801d320&gt; . Analysis from 1968 to 2019&#182; . # What is the average age of Grand Slams&#39; players from 1968 up to 2019? tennis_df_win=tennis_df[tennis_df[&#39;tourney_level&#39;].isin([&#39;G&#39;])&amp;(tennis_df[&#39;round&#39;]==&#39;F&#39;)].dropna(subset=[&#39;winner_age&#39;]) dfw = tennis_df_win[[&#39;tourney_year&#39;,&#39;tourney_name&#39;,&#39;winner_name&#39;,&#39;winner_age&#39;]] dfw.columns = [&#39;tourney_year&#39;,&#39;tourney_name&#39;,&#39;player&#39;,&#39;age&#39;] dfs_final = dfw.groupby([&#39;tourney_year&#39;,&#39;tourney_name&#39;]).agg({&#39;age&#39;:&#39;mean&#39;}).reset_index() dfs_final_2 = dfs_final.groupby([&#39;tourney_year&#39;]).agg({&#39;age&#39;:&#39;mean&#39;}).reset_index() fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) ax.set_prop_cycle(plt.cycler(&#39;color&#39;, plt.cm.jet(np.linspace(0, 1, 5)))) plt.title(&#39;Age of Grand Slams Champions 1968-2019&#39;) plt.ylabel(&#39;Age&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(dfs_final_2[&#39;tourney_year&#39;], dfs_final_2[&#39;age&#39;]) plt.scatter(dfs_final[dfs_final[&#39;tourney_name&#39;]==&#39;Australian Open&#39;][&#39;tourney_year&#39;], dfs_final[dfs_final[&#39;tourney_name&#39;]==&#39;Australian Open&#39;][&#39;age&#39;], alpha=0.3) plt.scatter(dfs_final[dfs_final[&#39;tourney_name&#39;]==&#39;Roland Garros&#39;][&#39;tourney_year&#39;], dfs_final[dfs_final[&#39;tourney_name&#39;]==&#39;Roland Garros&#39;][&#39;age&#39;], alpha=0.3) plt.scatter(dfs_final[dfs_final[&#39;tourney_name&#39;]==&#39;Wimbledon&#39;][&#39;tourney_year&#39;], dfs_final[dfs_final[&#39;tourney_name&#39;]==&#39;Wimbledon&#39;][&#39;age&#39;], alpha=0.3) plt.scatter(dfs_final[dfs_final[&#39;tourney_name&#39;]==&#39;US Open&#39;][&#39;tourney_year&#39;], dfs_final[dfs_final[&#39;tourney_name&#39;]==&#39;US Open&#39;][&#39;age&#39;], alpha=0.3) plt.legend([&#39;All Grand Slams Avg.&#39;,&#39;Australian Open&#39;, &#39;Roland Garros&#39;, &#39;Wimbledon&#39;, &#39;US Open&#39;], loc=&#39;upper center&#39;,prop={&#39;size&#39;: 9}) plt.xticks(np.arange(1968, 2019, 5), np.arange(1968, 2019, 5)) . ([&lt;matplotlib.axis.XTick at 0x7f6156773198&gt;, &lt;matplotlib.axis.XTick at 0x7f61567f29e8&gt;, &lt;matplotlib.axis.XTick at 0x7f61567f2710&gt;, &lt;matplotlib.axis.XTick at 0x7f61544439b0&gt;, &lt;matplotlib.axis.XTick at 0x7f6154443e80&gt;, &lt;matplotlib.axis.XTick at 0x7f61543cc390&gt;, &lt;matplotlib.axis.XTick at 0x7f61543cc860&gt;, &lt;matplotlib.axis.XTick at 0x7f61543ccd30&gt;, &lt;matplotlib.axis.XTick at 0x7f61543d4278&gt;, &lt;matplotlib.axis.XTick at 0x7f61543d4710&gt;, &lt;matplotlib.axis.XTick at 0x7f61543cc780&gt;], &lt;a list of 11 Text xticklabel objects&gt;) . Retirements&#182; . What is the evolution of retirements over time? In which tournament we see most of these retirements? . # Extract retirements ret_df = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==True][[&#39;tourney_year&#39;,&#39;tourney_level&#39;,&#39;surface&#39;,&#39;tourney_id&#39;,&#39;winner_name&#39;]] ref_df_f = ret_df.groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg(&#39;count&#39;) fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) plt.style.use(&#39;seaborn-colorblind&#39;) plt.title(&#39;Retirements - Evolution of Retirements by Surface&#39;) plt.ylabel(&#39;Number of Retirements&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Hard&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Hard&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, linewidth=2) plt.plot(ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Grass&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Grass&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=2) plt.plot(ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Clay&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Clay&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=2) plt.plot(ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Carpet&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Carpet&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=2) # Calc the trendline for hard court x = ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Hard&#39;][&#39;tourney_year&#39;].astype(int) y = ref_df_f[ref_df_f[&#39;surface&#39;]==&#39;Hard&#39;][&#39;tourney_id&#39;].astype(int) z = np.polyfit(x, y, 1) p = np.poly1d(z) plt.plot(x, p(x),&quot;r--&quot;, alpha=0.2) plt.legend([&#39;Hard&#39;, &#39;Grass&#39;, &#39;Clay&#39;, &#39;Carpet&#39;], loc=&#39;upper left&#39;, prop={&#39;size&#39;: 14}) plt.xticks(np.arange(1969, 2019, 5), np.arange(1968, 2019, 5)) . ([&lt;matplotlib.axis.XTick at 0x7f61544022e8&gt;, &lt;matplotlib.axis.XTick at 0x7f6154400b38&gt;, &lt;matplotlib.axis.XTick at 0x7f6154400860&gt;, &lt;matplotlib.axis.XTick at 0x7f61543b3c50&gt;, &lt;matplotlib.axis.XTick at 0x7f61543b3a90&gt;, &lt;matplotlib.axis.XTick at 0x7f61543be630&gt;, &lt;matplotlib.axis.XTick at 0x7f61543beb00&gt;, &lt;matplotlib.axis.XTick at 0x7f61543bee80&gt;, &lt;matplotlib.axis.XTick at 0x7f61543464e0&gt;, &lt;matplotlib.axis.XTick at 0x7f61543bed30&gt;], &lt;a list of 10 Text xticklabel objects&gt;) . Do the retirements by tournament type have the same behaviour? &#182; . ref_df_f = ret_df.groupby([&#39;tourney_year&#39;,&#39;tourney_level&#39;], as_index=False).agg(&#39;count&#39;) fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) plt.style.use(&#39;seaborn-colorblind&#39;) plt.title(&#39;Retirements - Evolution of Retirements by Tournament Type&#39;) plt.ylabel(&#39;Number of Retirements&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.plot(ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.plot(ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, linewidth=2, solid_capstyle=&#39;projecting&#39;) plt.plot(ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;D&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;D&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.plot(ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;F&#39;][&#39;tourney_year&#39;], ref_df_f[ref_df_f[&#39;tourney_level&#39;]==&#39;F&#39;][&#39;tourney_id&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.legend([&#39;Masters&#39;,&#39;Grand Slam&#39;, &#39;ATP Other&#39;, &#39;Davis Cup&#39;, &#39;Future&#39;], loc=&#39;upper left&#39;, prop={&#39;size&#39;: 14}) plt.xticks(np.arange(1968, 2019, 5), np.arange(1968, 2019, 5)) . ([&lt;matplotlib.axis.XTick at 0x7f61543704a8&gt;, &lt;matplotlib.axis.XTick at 0x7f6154372cf8&gt;, &lt;matplotlib.axis.XTick at 0x7f6154372a20&gt;, &lt;matplotlib.axis.XTick at 0x7f61543277b8&gt;, &lt;matplotlib.axis.XTick at 0x7f6154327240&gt;, &lt;matplotlib.axis.XTick at 0x7f61543327b8&gt;, &lt;matplotlib.axis.XTick at 0x7f6154332c88&gt;, &lt;matplotlib.axis.XTick at 0x7f615433b208&gt;, &lt;matplotlib.axis.XTick at 0x7f615433b668&gt;, &lt;matplotlib.axis.XTick at 0x7f615433bb38&gt;, &lt;matplotlib.axis.XTick at 0x7f615433be48&gt;], &lt;a list of 11 Text xticklabel objects&gt;) . Is is just that we have more retirements because there are more matches played in that tournament or surface? What if we consider the ratio of retirements over matches played? &#182; . ret_df_f = ret_df.groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg(&#39;count&#39;)[[&#39;tourney_year&#39;,&#39;surface&#39;,&#39;tourney_id&#39;]] ret_df_f.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;rets&#39;] notret_df = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==False][[&#39;tourney_year&#39;,&#39;surface&#39;,&#39;tourney_id&#39;]] notret_df_f = notret_df.groupby([&#39;tourney_year&#39;,&#39;surface&#39;], as_index=False).agg(&#39;count&#39;)[[&#39;tourney_year&#39;,&#39;surface&#39;,&#39;tourney_id&#39;]] notret_df_f.columns = [&#39;tourney_year&#39;,&#39;surface&#39;,&#39;norets&#39;] dfs = (ret_df_f, notret_df_f) dfs_concat = pd.concat(dfs, sort=False) dfs_c = dfs_concat.fillna(0).groupby([&#39;tourney_year&#39;,&#39;surface&#39;]).agg({&#39;rets&#39;:&#39;sum&#39;,&#39;norets&#39;:&#39;sum&#39;}).reset_index() dfs_c[&#39;ret_ratio&#39;] = np.where(dfs_c[&#39;norets&#39;]&gt;0, dfs_c[&#39;rets&#39;]/(dfs_c[&#39;rets&#39;]+dfs_c[&#39;norets&#39;]), 1) fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) plt.style.use(&#39;seaborn-colorblind&#39;) plt.title(&#39;Retirements - Evolution of Retirements by Surface&#39;) plt.yscale(&#39;log&#39;) # Using log scale plt.ylabel(&#39;Retirements ratio (log)&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(dfs_c[dfs_c[&#39;surface&#39;]==&#39;Hard&#39;][&#39;tourney_year&#39;], dfs_c[dfs_c[&#39;surface&#39;]==&#39;Hard&#39;][&#39;ret_ratio&#39;], linestyle=&#39;solid&#39;, linewidth=2, solid_capstyle=&#39;projecting&#39;) plt.plot(dfs_c[dfs_c[&#39;surface&#39;]==&#39;Grass&#39;][&#39;tourney_year&#39;], dfs_c[dfs_c[&#39;surface&#39;]==&#39;Grass&#39;][&#39;ret_ratio&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.plot(dfs_c[dfs_c[&#39;surface&#39;]==&#39;Clay&#39;][&#39;tourney_year&#39;], dfs_c[dfs_c[&#39;surface&#39;]==&#39;Clay&#39;][&#39;ret_ratio&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.legend([&#39;Hard&#39;,&#39;Grass&#39;, &#39;Clay&#39;], loc=&#39;upper left&#39;, prop={&#39;size&#39;: 14}) plt.xticks(np.arange(1968, 2019, 5), np.arange(1968, 2019, 5)) . ([&lt;matplotlib.axis.XTick at 0x7f61542f6be0&gt;, &lt;matplotlib.axis.XTick at 0x7f61542f6470&gt;, &lt;matplotlib.axis.XTick at 0x7f61542f6320&gt;, &lt;matplotlib.axis.XTick at 0x7f61542ac908&gt;, &lt;matplotlib.axis.XTick at 0x7f61542acdd8&gt;, &lt;matplotlib.axis.XTick at 0x7f61542b3320&gt;, &lt;matplotlib.axis.XTick at 0x7f61542b37b8&gt;, &lt;matplotlib.axis.XTick at 0x7f61542b3c88&gt;, &lt;matplotlib.axis.XTick at 0x7f61542bc208&gt;, &lt;matplotlib.axis.XTick at 0x7f61542bc668&gt;, &lt;matplotlib.axis.XTick at 0x7f61542b3748&gt;], &lt;a list of 11 Text xticklabel objects&gt;) . We can see there is a growth in retirements over the years. The growth seems to happen across surfaces. Does it also happen across tournament types? &#182; . ret_df_f = ret_df.groupby([&#39;tourney_year&#39;,&#39;tourney_level&#39;], as_index=False).agg(&#39;count&#39;)[[&#39;tourney_year&#39;,&#39;tourney_level&#39;,&#39;tourney_id&#39;]] ret_df_f.columns = [&#39;tourney_year&#39;,&#39;tourney_level&#39;,&#39;rets&#39;] notret_df = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==False][[&#39;tourney_year&#39;,&#39;tourney_level&#39;,&#39;tourney_id&#39;]] notret_df_f = notret_df.groupby([&#39;tourney_year&#39;,&#39;tourney_level&#39;], as_index=False).agg(&#39;count&#39;)[[&#39;tourney_year&#39;,&#39;tourney_level&#39;,&#39;tourney_id&#39;]] notret_df_f.columns = [&#39;tourney_year&#39;,&#39;tourney_level&#39;,&#39;norets&#39;] dfs = (ret_df_f, notret_df_f) dfs_concat = pd.concat(dfs, sort=False) dfs_c = dfs_concat.fillna(0).groupby([&#39;tourney_year&#39;,&#39;tourney_level&#39;]).agg({&#39;rets&#39;:&#39;sum&#39;,&#39;norets&#39;:&#39;sum&#39;}).reset_index() dfs_c[&#39;ret_ratio&#39;] = np.where(dfs_c[&#39;norets&#39;]&gt;0, dfs_c[&#39;rets&#39;]/(dfs_c[&#39;rets&#39;]+dfs_c[&#39;norets&#39;]), 1) fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) plt.style.use(&#39;seaborn-colorblind&#39;) plt.title(&#39;Retirements - Evolution of Retirements by Tournament Type&#39;) plt.yscale(&#39;log&#39;) # Using log scale plt.ylabel(&#39;Retirements ratio (log)&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(dfs_c[dfs_c[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;tourney_year&#39;], dfs_c[dfs_c[&#39;tourney_level&#39;]==&#39;G&#39;][&#39;ret_ratio&#39;], linestyle=&#39;solid&#39;, linewidth=2, solid_capstyle=&#39;projecting&#39;) plt.plot(dfs_c[dfs_c[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;tourney_year&#39;], dfs_c[dfs_c[&#39;tourney_level&#39;]==&#39;M&#39;][&#39;ret_ratio&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.plot(dfs_c[dfs_c[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;tourney_year&#39;], dfs_c[dfs_c[&#39;tourney_level&#39;]==&#39;A&#39;][&#39;ret_ratio&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.legend([&#39;Grand Slam&#39;,&#39;Masters&#39;, &#39;ATP Other&#39;], loc=&#39;upper left&#39;, prop={&#39;size&#39;: 14}) plt.xticks(np.arange(1968, 2016, 5), np.arange(1968, 2016, 5)) . ([&lt;matplotlib.axis.XTick at 0x7f615419ff98&gt;, &lt;matplotlib.axis.XTick at 0x7f615419f828&gt;, &lt;matplotlib.axis.XTick at 0x7f615419f550&gt;, &lt;matplotlib.axis.XTick at 0x7f615414add8&gt;, &lt;matplotlib.axis.XTick at 0x7f6154151320&gt;, &lt;matplotlib.axis.XTick at 0x7f615414a6a0&gt;, &lt;matplotlib.axis.XTick at 0x7f6154151128&gt;, &lt;matplotlib.axis.XTick at 0x7f6154151c18&gt;, &lt;matplotlib.axis.XTick at 0x7f61541517b8&gt;, &lt;matplotlib.axis.XTick at 0x7f615415a5f8&gt;], &lt;a list of 10 Text xticklabel objects&gt;) . If we look at the retirements ratio by tournament type we can see there has been a higher ratio of retirements in Grand Slams comparing to Masters and other ATP tournaments since the 1990s. . We can see that the number of retirements effectively increased since the 1990s, but why? Tennis has become more physical and competitive over the years and that has had an impact in players&#39; endurance and increase the probability of injuries. Considering this, older players are more prone to retire. . One way of validating this is checking if there is a relation between the retirements and the age of the player that retired. Let&#39;s check. . #df_minutes = tennis_df_all[pd.isnull(tennis_df_all[&#39;minutes&#39;])==False] df_ret_age = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==True].groupby([&#39;tourney_year&#39;]).agg({&#39;loser_age&#39;:&#39;mean&#39;}).reset_index() df_ret_age.columns = [&#39;tourney_year&#39;,&#39;age_ret&#39;] df_notret_age = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==False].groupby([&#39;tourney_year&#39;]).agg({&#39;loser_age&#39;:&#39;mean&#39;}).reset_index() df_notret_age.columns = [&#39;tourney_year&#39;,&#39;age_notret&#39;] df_age_all = df_ret_age.merge(df_notret_age, on=&#39;tourney_year&#39;, how=&#39;left&#39;) fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) plt.style.use(&#39;seaborn-colorblind&#39;) plt.title(&#39;Retirements - Age of Players that Retired vs. Winners&#39;) plt.ylabel(&#39;Player Age&#39;) plt.xlabel(&#39;Year&#39;) plt.plot(df_age_all[&#39;tourney_year&#39;], df_age_all[&#39;age_ret&#39;], linestyle=&#39;solid&#39;, linewidth=2, solid_capstyle=&#39;projecting&#39;) plt.plot(df_age_all[&#39;tourney_year&#39;], df_age_all[&#39;age_notret&#39;], linestyle=&#39;solid&#39;, marker=&#39;o&#39;, markerfacecolor=&#39;black&#39;, markersize=1, linewidth=3) plt.legend([&#39;Age Retired&#39;,&#39;Age Winner&#39;], loc=&#39;upper center&#39;, prop={&#39;size&#39;: 14}) plt.xticks(np.arange(1968, 2019, 5), np.arange(1968, 2019, 5)) . ([&lt;matplotlib.axis.XTick at 0x7f61540e4b38&gt;, &lt;matplotlib.axis.XTick at 0x7f61540e43c8&gt;, &lt;matplotlib.axis.XTick at 0x7f61540e4278&gt;, &lt;matplotlib.axis.XTick at 0x7f6154094278&gt;, &lt;matplotlib.axis.XTick at 0x7f6154094710&gt;, &lt;matplotlib.axis.XTick at 0x7f6154094be0&gt;, &lt;matplotlib.axis.XTick at 0x7f6154094550&gt;, &lt;matplotlib.axis.XTick at 0x7f615409c5c0&gt;, &lt;matplotlib.axis.XTick at 0x7f615409ca90&gt;, &lt;matplotlib.axis.XTick at 0x7f615409cf60&gt;, &lt;matplotlib.axis.XTick at 0x7f61540a2470&gt;], &lt;a list of 11 Text xticklabel objects&gt;) . If we look at countries with the most matches in the ATP, which ones are the ones with the highest retirements&#39; ratio? &#182; . df_ret_cntry = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==True].groupby([&#39;loser_ioc&#39;,&#39;tourney_level&#39;]).agg(&#39;count&#39;).reset_index()[[&#39;loser_ioc&#39;,&#39;tourney_level&#39;,&#39;tourney_id&#39;]] df_ret_cntry.columns = [&#39;country&#39;,&#39;tourney_level&#39;,&#39;total_ret&#39;] df_notret_cntry = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==False].groupby([&#39;loser_ioc&#39;,&#39;tourney_level&#39;]).agg(&#39;count&#39;).reset_index()[[&#39;loser_ioc&#39;,&#39;tourney_level&#39;,&#39;tourney_id&#39;]] df_notret_cntry.columns = [&#39;country&#39;,&#39;tourney_level&#39;,&#39;total_noret&#39;] df_cntry_all = df_ret_cntry.merge(df_notret_cntry, on=[&#39;country&#39;,&#39;tourney_level&#39;], how=&#39;left&#39;) df_cntry_all[&#39;ret_ratio&#39;] = df_cntry_all[&#39;total_ret&#39;]/(df_cntry_all[&#39;total_noret&#39;]+df_cntry_all[&#39;total_ret&#39;]) df_cntry_final = df_cntry_all[df_cntry_all[&#39;country&#39;].isin([&#39;ARG&#39;,&#39;ESP&#39;,&#39;USA&#39;,&#39;SUI&#39;,&#39;SRB&#39;])].sort_values([&#39;ret_ratio&#39;], ascending=False) ax=sns.boxplot(x=&quot;country&quot;, y=&quot;ret_ratio&quot;, data=df_cntry_final) ax.set(xlabel=&#39;Country&#39;, ylabel=&#39;Ret. Ratio&#39;, title=&#39;Highest Retirement Ratio by Country&#39;) . [Text(0, 0.5, &#39;Ret. Ratio&#39;), Text(0.5, 0, &#39;Country&#39;), Text(0.5, 1.0, &#39;Highest Retirement Ratio by Country&#39;)] . Who are the players that retired the most? Not looking at ratios here, just the total number of retirements &#182; . df_ret_players = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==True].groupby([&#39;loser_name&#39;,&#39;loser_ioc&#39;]).agg(&#39;count&#39;).reset_index()[[&#39;loser_name&#39;,&#39;loser_ioc&#39;,&#39;tourney_id&#39;]] df_ret_players.columns = [&#39;player_name&#39;,&#39;country&#39;,&#39;total_ret&#39;] df_ret_players = df_ret_players.sort_values([&#39;total_ret&#39;],ascending=False).head(10) df_ret_players_sorted = df_ret_players.sort_values([&#39;total_ret&#39;],ascending=True) df_ret_players_sorted.plot(&#39;player_name&#39;,&#39;total_ret&#39;, kind=&#39;barh&#39;, title=&#39;Players with most Retirements&#39;, legend=False) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7f6150e436d8&gt; . What about the players that played the most matches and never retired from a match? What was their winning ratio? &#182; . # Wins for each player df_win_players = tennis_df.groupby([&#39;winner_name&#39;]).agg({&#39;tourney_id&#39;:&#39;count&#39;,&#39;tourney_year&#39;:&#39;max&#39;}).reset_index()[[&#39;winner_name&#39;,&#39;tourney_id&#39;,&#39;tourney_year&#39;]] df_win_players.columns = [&#39;player_name&#39;,&#39;total_won&#39;,&#39;tourney_year&#39;] df_win_players[&#39;year_period&#39;] = pd.cut(df_win_players.tourney_year.astype(int), [1968, 1979, 1989, 1999, 2009, np.inf], labels=[&#39;1970s&#39;,&#39;1980s&#39;,&#39;1990s&#39;,&#39;2000s&#39;,&#39;2010s&#39;]) # Loses for each player df_lost_players = tennis_df.groupby([&#39;loser_name&#39;]).agg({&#39;tourney_id&#39;:&#39;count&#39;,&#39;tourney_year&#39;:&#39;max&#39;}).reset_index()[[&#39;loser_name&#39;,&#39;tourney_id&#39;,&#39;tourney_year&#39;]] df_lost_players.columns = [&#39;player_name&#39;,&#39;total_lost&#39;,&#39;tourney_year&#39;] df_lost_players[&#39;year_period&#39;] = pd.cut(df_lost_players.tourney_year.astype(int), [1968, 1979, 1989, 1999, 2009, np.inf], labels=[&#39;1970s&#39;,&#39;1980s&#39;,&#39;1990s&#39;,&#39;2000s&#39;, &#39;2010s&#39;]) # Retirements for each player df_ret_players = tennis_df[tennis_df[&#39;score&#39;].str.contains(&quot;RET&quot;)==True].groupby([&#39;loser_name&#39;]).agg({&#39;tourney_id&#39;:&#39;count&#39;,&#39;tourney_year&#39;:&#39;max&#39;}).reset_index()[[&#39;loser_name&#39;,&#39;tourney_id&#39;,&#39;tourney_year&#39;]] df_ret_players.columns = [&#39;player_name&#39;,&#39;total_ret&#39;,&#39;tourney_year&#39;] df_ret_players[&#39;year_period&#39;] = pd.cut(df_ret_players.tourney_year.astype(int), [1968, 1979, 1989, 1999, 2009, np.inf], labels=[&#39;1970s&#39;,&#39;1980s&#39;,&#39;1990s&#39;,&#39;2000s&#39;, &#39;2010s&#39;]) # Concatenate data frames dfs_players = (df_win_players, df_lost_players, df_ret_players) dfs_players_concat = pd.concat(dfs_players, sort=False) # Group by player name and year period dfs_players_final = dfs_players_concat.groupby([&#39;player_name&#39;,&#39;year_period&#39;]).agg({&#39;total_won&#39;:&#39;sum&#39;,&#39;total_lost&#39;:&#39;sum&#39;,&#39;total_ret&#39;:&#39;sum&#39;}).fillna(0).reset_index() dfs_players_final[&#39;total_played&#39;] = dfs_players_final[&#39;total_won&#39;]+dfs_players_final[&#39;total_lost&#39;]+dfs_players_final[&#39;total_ret&#39;] dfs_players_final[&#39;win_ratio&#39;] = dfs_players_final[&#39;total_won&#39;]/(dfs_players_final[&#39;total_won&#39;]+dfs_players_final[&#39;total_lost&#39;]+dfs_players_final[&#39;total_ret&#39;]) dfs_players_top = dfs_players_final[dfs_players_final[&#39;total_ret&#39;]==0].sort_values([&#39;total_played&#39;],ascending=False).head(20) f, ax = plt.subplots(figsize=(6, 8)) # Plot player names with total matches played sns.set_color_codes(&quot;pastel&quot;) sns.barplot(x=&quot;total_played&quot;, y=&quot;player_name&quot;, data=dfs_players_top, label=&quot;Total Played&quot;, color=&quot;b&quot;) # Plot player names with total wins sns.set_color_codes(&quot;muted&quot;) sns.barplot(x=&quot;total_won&quot;, y=&quot;player_name&quot;, data=dfs_players_top, label=&quot;Wins&quot;, color=&quot;b&quot;) ax.legend(ncol=1, loc=&quot;lower right&quot;, frameon=True) ax.set(xlim=(0, 1500), ylabel=&quot;Top 50 players&quot;, xlabel=&quot;Matches&quot;, title=&quot;Players&#39; win ratios with most matches without retirements&quot;) sns.despine(left=True, bottom=True) . All these players are from different time periods. Let&#39;s see, how many of the top 10 were actually playing together. &#182; . ax=sns.catplot(x=&quot;total_played&quot;,y=&quot;player_name&quot;, col=&quot;year_period&quot;, data=dfs_players_top.head(10), kind=&quot;bar&quot;, height=5, aspect=.7); ax.set(xlabel=&quot;Total&quot;, ylabel=&quot;Player Name&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f614dcd6f98&gt; . Longest career spans&#182; . Which players have had the longest career spans? At what age did they stop playing? . # Calculate age, min and max year for players df_winnners_bio = tennis_df.groupby([&#39;winner_name&#39;]).agg({&#39;winner_age&#39;:&#39;max&#39;,&#39;tourney_year&#39;:[&#39;min&#39;,&#39;max&#39;]}).reset_index().fillna(0) df_winnners_bio.columns = [&#39;player_name&#39;,&#39;age&#39;, &#39;year_min&#39;, &#39;year_max&#39;] df_losers_bio = tennis_df.groupby([&#39;loser_name&#39;]).agg({&#39;loser_age&#39;:&#39;max&#39;,&#39;tourney_year&#39;:[&#39;min&#39;,&#39;max&#39;]}).reset_index().fillna(0) df_losers_bio.columns = [&#39;player_name&#39;,&#39;age&#39;, &#39;year_min&#39;, &#39;year_max&#39;] dfs_bio = (df_winnners_bio, df_losers_bio) dfs_bio_concat = pd.concat(dfs_bio, sort=False) dfs_bio_final = dfs_bio_concat.groupby([&#39;player_name&#39;]).agg({&#39;age&#39;:&#39;max&#39;,&#39;year_min&#39;:&#39;min&#39;, &#39;year_max&#39; : &#39;max&#39;}).reset_index() # Calculate active years for players df_winners_years = tennis_df.groupby([&#39;winner_name&#39;,&#39;tourney_year&#39;]).agg({&#39;tourney_id&#39;:&#39;count&#39;}).reset_index()#.groupby([&#39;winner_name&#39;]).agg({&#39;tourney_year&#39;:&#39;count&#39;}).sort_values([&#39;tourney_year&#39;], ascending=False) df_winners_years.columns = [&#39;player_name&#39;,&#39;tourney_year&#39;,&#39;count&#39;] df_losers_years = tennis_df.groupby([&#39;loser_name&#39;,&#39;tourney_year&#39;]).agg({&#39;tourney_id&#39;:&#39;count&#39;}).reset_index()#.groupby([&#39;winner_name&#39;]).agg({&#39;tourney_year&#39;:&#39;count&#39;}).sort_values([&#39;tourney_year&#39;], ascending=False) df_losers_years.columns = [&#39;player_name&#39;,&#39;tourney_year&#39;,&#39;count&#39;] dfs = (df_winners_years, df_losers_years) dfs_concat = pd.concat(dfs) df_player_years = dfs_concat.groupby([&#39;player_name&#39;,&#39;tourney_year&#39;]).agg({&#39;tourney_year&#39;:&#39;count&#39;}).groupby([&#39;player_name&#39;]).agg({&#39;tourney_year&#39;:&#39;count&#39;}).reset_index().sort_values([&#39;tourney_year&#39;], ascending=False).head(10) df_player_final = df_player_years.merge(dfs_bio_final, on=&#39;player_name&#39;) # Exclude last year because they may still be playing df_player_final = df_player_final[df_player_final[&#39;year_max&#39;].astype(int) &lt; 2016] df_player_final[&#39;player_lname&#39;] = df_player_final[&#39;player_name&#39;].str.split(&quot; &quot;).str.get(1) f, ax1 = plt.subplots(figsize=(20, 5)) ax2 = ax1.twinx() df_player_final.plot(ax=ax2, x=&#39;player_lname&#39;, y=&#39;age&#39;, marker=&#39;o&#39;, color=&#39;r&#39;, linewidth=&#39;5&#39;, title=&#39;Players with longest career spans&#39;) df_player_final.plot(ax=ax1, x=&#39;player_lname&#39;, y=&#39;tourney_year&#39;, kind=&#39;bar&#39;, legend=False) ax1.set_xlabel(&#39;Player Last Name&#39;) ax1.set_ylabel(&#39;Years Played&#39;) ax2.set_ylabel(&#39;Age&#39;) . Text(0, 0.5, &#39;Age&#39;) . Head to Head&#182; . Which players played the most head to head matches per time period? . h2h_wl = tennis_df.groupby([&#39;winner_name&#39;,&#39;loser_name&#39;]).agg({&#39;tourney_id&#39;:&#39;count&#39;,&#39;tourney_year&#39;:&#39;max&#39;}).reset_index() h2h_wl.columns = [&#39;player_a&#39;,&#39;player_b&#39;,&#39;total&#39;,&#39;year&#39;] h2h_lw = tennis_df.groupby([&#39;loser_name&#39;,&#39;winner_name&#39;]).agg({&#39;tourney_id&#39;:&#39;count&#39;,&#39;tourney_year&#39;:&#39;max&#39;}).reset_index() h2h_lw.columns = [&#39;player_a&#39;,&#39;player_b&#39;,&#39;total&#39;,&#39;year&#39;] h2h_f = h2h_wl.merge(h2h_lw, on=[&#39;player_a&#39;, &#39;player_b&#39;]) h2h_f[&#39;total&#39;] = h2h_f[&#39;total_x&#39;] + h2h_f[&#39;total_y&#39;] h2h_f[&#39;player_a&#39;] = np.where(h2h_f[&#39;player_a&#39;] &lt; h2h_f[&#39;player_b&#39;], h2h_f[&#39;player_a&#39;], h2h_f[&#39;player_b&#39;]) h2h_f[&#39;player_b&#39;] = np.where(h2h_f[&#39;player_a&#39;] &gt; h2h_f[&#39;player_b&#39;], h2h_f[&#39;player_a&#39;], h2h_f[&#39;player_b&#39;]) h2h_f[&#39;year&#39;] = np.where(h2h_f[&#39;year_x&#39;] &gt; h2h_f[&#39;year_y&#39;], h2h_f[&#39;year_x&#39;], h2h_f[&#39;year_y&#39;]) h2h_f[&#39;names&#39;] = h2h_f[&#39;player_a&#39;].str.split(&quot; &quot;).str.get(1) + &quot;-&quot; + h2h_f[&#39;player_b&#39;].str.split(&quot; &quot;).str.get(1) h2h_f2 = h2h_f.groupby([&#39;player_a&#39;,&#39;player_b&#39;,&#39;names&#39;]).agg({&#39;total&#39;:&#39;max&#39;,&#39;year&#39;:&#39;max&#39;}).reset_index() h2h_f2_sorted = h2h_f2[h2h_f2[&#39;player_a&#39;]!=h2h_f2[&#39;player_b&#39;]].sort_values([&#39;total&#39;], ascending=False)#.head(20) h2h_f2_sorted[&#39;year_period&#39;] = pd.cut(h2h_f2_sorted.year.astype(int), [1968, 1979, 1989, 1999, 2009, np.inf], labels=[&#39;1970s&#39;,&#39;1980s&#39;,&#39;1990s&#39;,&#39;2000s&#39;, &#39;2010s&#39;]) #h2h_f2_sorted #f, ax = plt.subplots(figsize=(25, 15)) plt.figure(figsize=(20,25)) plt.subplot(5,1,1) ax1=sns.barplot(x=&quot;total&quot;, y=&quot;names&quot;, palette=&#39;Blues_d&#39;, data=h2h_f2_sorted[h2h_f2_sorted.year_period==&#39;1970s&#39;].head(10)) ax1.set(xlabel=&#39;&#39;, ylabel=&#39;Players&#39;, title=&#39;Top 10 rivalries in 1970s&#39;) plt.subplot(5,1,2) ax2=sns.barplot(x=&quot;total&quot;, y=&quot;names&quot;, palette=&#39;Blues_d&#39;, data=h2h_f2_sorted[h2h_f2_sorted.year_period==&#39;1980s&#39;].head(10)) ax2.set(xlabel=&#39;&#39;, ylabel=&#39;Players&#39;, title=&#39;Top 10 rivalries in 1980s&#39;) plt.subplot(5,1,3) ax3=sns.barplot(x=&quot;total&quot;, y=&quot;names&quot;, palette=&#39;Blues_d&#39;, data=h2h_f2_sorted[h2h_f2_sorted.year_period==&#39;1990s&#39;].head(10)) ax3.set(xlabel=&#39;&#39;, ylabel=&#39;Players&#39;, title=&#39;Top 10 rivalries in 1990s&#39;) plt.subplot(5,1,4) ax4=sns.barplot(x=&quot;total&quot;, y=&quot;names&quot;, palette=&#39;Blues_d&#39;, data=h2h_f2_sorted[h2h_f2_sorted.year_period==&#39;2000s&#39;].head(10)) ax4.set(xlabel=&#39;&#39;, ylabel=&#39;Players&#39;, title=&#39;Top 10 rivalries in 2000s&#39;) plt.subplot(5,1,5) ax5=sns.barplot(x=&quot;total&quot;, y=&quot;names&quot;, palette=&#39;Blues_d&#39;, data=h2h_f2_sorted[h2h_f2_sorted.year_period==&#39;2010s&#39;].head(10)) ax5.set(xlabel=&#39;&#39;, ylabel=&#39;Players&#39;, title=&#39;Top 10 rivalries in 2010s&#39;) sns.despine(left=True, bottom=True) .",
            "url": "https://www.dataguasu.com/blog/tennis/eda/2018/10/12/data-visualizations-of-atp-tennis-competitions.html",
            "relUrl": "/tennis/eda/2018/10/12/data-visualizations-of-atp-tennis-competitions.html",
            "date": " • Oct 12, 2018"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.dataguasu.com/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "",
          "content": "Warning . Do not manually save images into this folder. This is used by GitHub Actions to automatically copy images. Any images you save into this folder could be deleted at build time. .",
          "url": "https://www.dataguasu.com/blog/images/copied_from_nb/",
          "relUrl": "/images/copied_from_nb/",
          "date": ""
      }
      
  

  
  

}