{
  
    
        "post0": {
            "title": "Introducing fastpages",
            "content": ". We are very pleased to announce the immediate availability of fastpages. fastpages is a platform which allows you to create and host a blog for free, with no ads and many useful features, such as: . Create posts containing code, outputs of code (which can be interactive), formatted text, etc directly from Jupyter Notebooks; for instance see this great example post from Scott Hawley. Notebook posts support features such as: Interactive visualizations made with Altair remain interactive. | Hide or show cell input and output. | Collapsable code cells that are either open or closed by default. | Define the Title, Summary and other metadata via a special markdown cells | Ability to add links to Colab and GitHub automatically. | . | Create posts, including formatting and images, directly from Microsoft Word documents. | Create and edit Markdown posts entirely online using GitHub&#39;s built-in markdown editor. | Embed Twitter cards and YouTube videos. | Categorization of blog posts by user-supplied tags for discoverability. | ... and much more | . fastpages relies on Github pages for hosting, and Github Actions to automate the creation of your blog. The setup takes around three minutes, and does not require any technical knowledge or expertise. Due to built-in automation of fastpages, you don&#39;t have to fuss with conversion scripts. All you have to do is save your Jupyter notebook, Word document or markdown file into a specified directory and the rest happens automatically. Infact, this blog post is written in a Jupyter notebook, which you can see with the &quot;View on GitHub&quot; link above. . fast.ai have previously released a similar project called fast_template, which is even easier to set up, but does not support automatic creation of posts from Microsoft Word or Jupyter notebooks, including many of the features outlined above. . Because fastpages is more flexible and extensible, we recommend using it where possible. fast_template may be a better option for getting folks blogging who have no technical expertise at all, and will only be creating posts using Github&#39;s integrated online editor. . Setting Up Fastpages&#182; . The setup process of fastpages is automated with GitHub Actions, too! Upon creating a repo from the fastpages template, a pull request will automatically be opened (after ~ 30 seconds) configuring your blog so it can start working. The automated pull request will greet you with instructions like this: . . All you have to do is follow these instructions (in the PR you receive) and your new blogging site will be up and running! . Jupyter Notebooks &amp; Fastpages&#182; . In this post, we will cover special features that fastpages provides has for Jupyter notebooks. You can also write your blog posts with Word documents or markdown in fastpages, which contain many, but not all the same features. . Options via FrontMatter&#182; . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . All of the above settings are enabled in this post, so you can see what they look like! . the summary field (preceeded by &gt;) will be displayed under your title, and will also be used by social media to display as the description of your page. | toc: setting this to true will automatically generate a table of contents | badges: setting this to true will display Google Colab and GitHub links on your blog post. | comments: setting this to true will enable comments. See these instructions for more details. | author this will display the authors names. | categories will allow your post to be categorized on a &quot;Tags&quot; page, where readers can browse your post by categories. | . Markdown front matter is formatted similarly to notebooks. The differences between the two can be viewed on the fastpages README. . Code Folding&#182; . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . If you want to completely hide cells (not just collapse them), read these instructions . Interactive Charts With Altair&#182; . Interactive visualizations made with Altair remain interactive! . We leave this below cell unhidden so you can enjoy a preview of syntax highlighting in fastpages, which uses the Dracula theme. . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables&#182; . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;IMDB_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget IMDB_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | 6.1 | . 1 First Love, Last Rites | 10876.0 | 300000.0 | 6.9 | . 2 I Married a Strange Person | 203134.0 | 250000.0 | 6.8 | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | NaN | . 4 Slam | 1087521.0 | 1000000.0 | 3.4 | . Images w/Captions&#182; . You can include markdown images with captions like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Of course, the caption is optional. . Tweetcards&#182; . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos&#182; . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts&#182; . Typing &gt; Warning: There will be no second warning! will render this: Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: Note: A doc link to an example website: fast.ai should also work fine. . More Examples&#182; . This tutorial contains more examples of what you can do with notebooks. . How fastpages Converts Notebooks to Blog Posts&#182; . fastpages uses nbdev to power the conversion process of Jupyter Notebooks to blog posts. When you save a notebook into the /_notebooks folder of your repository, GitHub Actions applies nbdev against those notebooks automatically. The same process occurs when you save Word documents or markdown files into the _word or _posts directory, respectively. . We will discuss how GitHub Actions work in a follow up blog post. . Resources &amp; Next Steps&#182; . We highly encourage you to start blogging with fastpages! Some resources that may be helpful: . fastpages repo - this is where you can go to create your own fastpages blog! | Fastai forums - nbdev &amp; blogging category. You can ask questions about fastpages here, as well as suggest new features. | nbdev: this project powers the conversion of Jupyter notebooks to blog posts. | . If you end up writing a blog post using fastpages, please let us know on Twitter: @jeremyphoward, @HamelHusain. .",
            "url": "https://www.dataguasu.com/blog/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "relUrl": "/fastpages/jupyter/2020/02/21/introducing-fastpages.html",
            "date": " • Feb 21, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About&#182; . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter&#182; . Front Matter is a markdown cell at the beginning of your notebook that allows you to inject metadata into your notebook. For example: . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts&#182; . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair&#182; . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown&#182; . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips&#182; . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips&#182; . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables&#182; . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images&#182; . Local Images&#182; . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images&#182; . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs&#182; . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions&#182; . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements&#182; . Tweetcards&#182; . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos&#182; . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts&#182; . Typing &gt; Warning: There will be no second warning! will render this: Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: Note: A doc link to an example website: fast.ai should also work fine. . Footnotes&#182; . You can have footnotes in notebooks just like you can with markdown. . For example, here is a footnote 1. . . This is the footnote.&#8617; . |",
            "url": "https://www.dataguasu.com/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Example Markdown Post",
            "content": "Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . You can include alert boxes …and… . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://www.dataguasu.com/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Reclamos ciudadanos en Twitter",
            "content": "Introducci&#243;n&#182; . Desde hace ya unos años, las redes sociales se volvieron el medio de comunicación preferido de los ciudadanos para realizar reclamos relacionados a la provisión de servicios públicos e infraestructura (electricidad, agua potable, recolección de basura, reportes de baches, etc.) . Si bien esto produjo un avance importante en la comunicación ciudadanía-autoridades, el exceso y la velocidad de generación de la información impide tener un análisis certero de los reclamos como para reaccionar de manera eficaz, entender la causa raíz y prevenir futuros eventos. . Entender esto nos ayudaría por ejemplo a responder las siguientes preguntas: . ¿Cuáles son los reclamos más frecuentes realizados por los ciudadanos? | ¿Cómo varía la intensidad de estos reclamos en el tiempo? | ¿Cuál es el sentimiento de las publicaciones realizadas por los ciudadanos hacia las autoridades? | . Con el objetivo de lograr un entendimiento mas profundo que nos permita responder a estas y otras preguntas, utilizo los tweets o publicaciones realizadas en Twitter donde se mencionan a la Municipalidad de Asunción (@AsuncionMuni) y al Intendente (@FerreiroMario1). . An&#225;lisis Exploratorio&#182; . Librerias&#182; . En primer lugar, importamos todas las librerias necesarias para el análisis. En este caso, decidí utilizar Pandas para la manipulación de los datos con los conocidos dataframes, seaborn y matplotlib para visualizaciones, NLTK para tokenización, en conjunto con sklearn para implementar técnicas como TF-IDF que son útiles para extraer las combinaciones de palabras más relevantes y como entrada para otras técnicas como LDA, que detallo más adelante. . import datetime, re, spacy, nltk import calendar from time import time from nltk.corpus import stopwords from nltk.tokenize.toktok import ToktokTokenizer from nltk.tokenize import TweetTokenizer from nltk import ngrams import string import pandas as pd import numpy as np from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator from PIL import Image from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer from sklearn.decomposition import LatentDirichletAllocation from sklearn.pipeline import Pipeline from collections import Counter import seaborn as sns import matplotlib.pyplot as plt %matplotlib inline . Carga de tweets a pandas dataframe&#182; . Los tweets se encuentran contenidos en un archivo cuya estructura se detalla en el apartado de datos en Kaggle. Además de cargar los tweets a un dataframe de pandas, derivo otros atributos que serán útiles más adelante. Extraer algunos tweets de ejemplo puede ser útil para entender los tipos de datos en cada columna. . df_tweets_muni = pd.read_csv(&quot;../input/tweets-municipalidad-asuncion/tweets_municipalidad.csv&quot;) df_tweets_muni[&#39;created_at&#39;] = pd.to_datetime(df_tweets_muni[&#39;created_at&#39;]) df_tweets_muni[&#39;date&#39;] = pd.to_datetime(df_tweets_muni[&#39;created_at&#39;].dt.date) df_tweets_muni[&#39;month&#39;] = df_tweets_muni[&#39;date&#39;].dt.month df_tweets_muni[&#39;year&#39;] = df_tweets_muni[&#39;date&#39;].dt.year df_tweets_muni[&#39;tweet&#39;] = df_tweets_muni[&#39;tweet&#39;].astype(str) df_tweets_muni[&#39;year_month&#39;] = df_tweets_muni[&#39;date&#39;].dt.to_period(&#39;M&#39;) . df_tweets_muni.head(5) . created_at tweet username date month year year_month . 0 2017-10-30 23:57:03+00:00 | exelente saludos desde nycity | renevera2013 | 2017-10-30 | 10 | 2017 | 2017-10 | . 1 2017-10-30 23:50:51+00:00 | Ahora mismo 5 autos &quot;agarran&quot; el carril derech... | Superman74Cacer | 2017-10-30 | 10 | 2017 | 2017-10 | . 2 2017-10-30 23:50:01+00:00 | Siempre la misma cosa frente a CASACOR... | Superman74Cacer | 2017-10-30 | 10 | 2017 | 2017-10 | . 3 2017-10-30 23:47:24+00:00 | ESAS PAREDES....SE TIENE QUE LLAMAR A LOS CAND... | aweissman1950 | 2017-10-30 | 10 | 2017 | 2017-10 | . 4 2017-10-30 23:46:34+00:00 | Hacen años que ahí no se puede girar a la izqu... | aristidesag | 2017-10-30 | 10 | 2017 | 2017-10 | . Deduplicaci&#243;n y filtrado&#182; . En las siguientes celdas, elimino los tweets duplicados que puedieron aparecer al realizar la extracción o en caso que usuarios hayan publicado el mismo tweet varias veces. . Luego, identifico los usuarios con más cantidad de tweets y excluyo aquellas cuentas relacionadas a la Municipalidad, medios de comunicación u otros entes públicos de tal manera a considerar únicamente las publicaciones de los ciudadanos. . # Borrar tweets duplicados (nos quedamos con el primero) df_tweets_muni = df_tweets_muni.drop_duplicates(subset=[&#39;tweet&#39;], keep=&#39;first&#39;) . # Usuarios con mas tweets df_tweets_username = df_tweets_muni.groupby(&#39;username&#39;).count().reset_index() df_tweets_username.sort_values(by=&#39;tweet&#39;, ascending=False)[[&#39;username&#39;,&#39;tweet&#39;]].head(10) . username tweet . 26216 pmtasuncion1 | 11679 | . 1413 AsuncionMuni | 4881 | . 1409 AsuDsu | 3273 | . 3830 El_Rafa_PY | 1984 | . 247 ABCCardinal | 1207 | . 7146 LaUnionAM | 1182 | . 194 780AM | 1071 | . 28362 teclitamovil | 841 | . 1408 AsuDgrrd | 731 | . 15991 chrispresspy | 722 | . # Excluir cuentas de la municipalidad o de medios de comunicacion filtro = ~(df_tweets_muni[&#39;username&#39;].isin([&#39;AsuncionMuni&#39;,&#39;pmtasuncion1&#39;,&#39;AsuDsu&#39;,&#39;ABCCardinal&#39;, &#39;LaUnionAM&#39;, &#39;780AM&#39;, &#39;AsuDgrrd&#39;, &#39;Universo970py&#39;, &#39;1000_am&#39;, &#39;EssapSA&#39;, &#39;Ferreiromario1&#39;, &#39;AbastoAsu&#39;, &#39;ANDEOficial&#39;, &#39;mopcparaguay&#39;])) # Crear copia de dataframe con filtro aplicado df_tweets_muni_filtro = df_tweets_muni[filtro].copy() . Limpieza&#182; . Los tweets publicados pueden tener información poco relevante para análisis textuales como por ejemplo URLs, emails, referencias a imagenes o simbolos. Además, existen palabras conocidas como stopwords que se repiten frecuentemente y no aportan al entendimiento de la conversación (el, la, y, con, para, mi, etc.). NLTK es una librería de NLP que incluye stopwords en diferentes idiomas, incluyendo español. . # Cargar stopwords en español stopwords_es = stopwords.words(&#39;spanish&#39;) &quot;&quot;&quot; Excluir menciones, emails, URLs y simbolos &quot;&quot;&quot; def clean_tweet(tweet): # Convertir a minusculas tweet = tweet.lower() # Excluir menciones o emails tweet = re.sub(r&#39; w*@( w+ .* w+ .* w+)&#39;,&#39; &#39;, tweet) # Excluir simbolos tweet = tweet.translate(str.maketrans(&#39;&#39;, &#39;&#39;, string.punctuation)) # Excluir URLs tweet = re.sub(r&#39;(?:www .|https?)[^ s]+&#39;, &#39; &#39;, tweet, flags=re.MULTILINE) # Borrar espacios tweet = tweet.strip() # Considerar solo valores alfa numericos tweet_alfa = re.compile(&quot;^(?![0-9]*$)[a-zA-Z0-9]+$&quot;) # Eliminar stopwords y palabras con longitud &lt;= 2 tokens = tweet.split() text = [token for token in tokens if token not in stopwords_es and len(token)&gt;2 and tweet_alfa.match(token)] return &#39; &#39;.join(text) # Aplicar filtro a tweets df_tweets_muni_filtro[&#39;tweet_cleaned&#39;] = df_tweets_muni_filtro[&#39;tweet&#39;].apply(clean_tweet) . Bigramas frecuentes por a&#241;o&#182; . Los bigramas son combinaciones de dos palabras que pueden dar una mejor idea de los temas de conversación. En este caso, me interesa conocer los bigramas que más se repiten y para ellos aplico técnicas de tokenización que separan las palabras del texto y cada par se convierte en una fila. También se puede modificar el tamaño del ngram para formar unigramas, trigramas, etc. El bigrama es una opción intermedia que permite tener algo más de contexto pero tiene suficientes ocurrencias para que sea significativa la muestra (mientras mayor sea el ngram, menor el número de ocurrencias). . # Tamano del ngram ngram = 2 tokenizer = TweetTokenizer() # Tokenizar y aplicar ngram df_tweets_muni_filtro[&#39;tokenize&#39;] = df_tweets_muni_filtro[&#39;tweet_cleaned&#39;] .apply(tokenizer.tokenize) df_tweets_muni_filtro[&#39;ngram&#39;] = df_tweets_muni_filtro[&#39;tokenize&#39;] .apply(lambda x: list(ngrams(x, ngram))) # Una fila por ngram df_tweets_muni_exploded = df_tweets_muni_filtro .explode(&#39;ngram&#39;)[[&#39;date&#39;,&#39;year&#39;,&#39;ngram&#39;]] . /opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: generator &#39;ngrams&#39; raised StopIteration if __name__ == &#39;__main__&#39;: . # Agrupar por cantidad de ocurrencias df_tweets_muni_exploded_grouped = df_tweets_muni_exploded .groupby([&#39;year&#39;, &#39;ngram&#39;]) .agg({&#39;date&#39;:&#39;count&#39;}) .reset_index() .sort_values(by=[&#39;year&#39;, &#39;date&#39;], ascending=False) .rename(columns={&#39;date&#39;:&#39;count&#39;}) # Top 10 por YYYY df_tweets_muni_top_year = df_tweets_muni_exploded_grouped .drop_duplicates(subset=[&#39;count&#39;,&#39;year&#39;]) .groupby([&#39;year&#39;]) .head(10) . La siguiente gráfica muestra las ocurrencias de cada bigrama por año. Se pueden identificar por ejemplo bigramas que aparecen en multiples años . import seaborn as sns ax = sns.catplot(x=&quot;count&quot;,y=&quot;ngram&quot;, col=&quot;year&quot;, data=df_tweets_muni_top_year, kind=&quot;bar&quot;, height=5, aspect=.7); ax.set(xlabel=&quot;Frequencia&quot;, ylabel=&quot;Bigrama&quot;) . &lt;seaborn.axisgrid.FacetGrid at 0x7f949202b978&gt; . Wordcloud&#182; . Las nubes de palabras o word clouds permiten visualizar las palabras más frecuentes de un texto utilizando el tamaño para representar la frecuencia o importancia. En este caso, las palabras se extraen de los tweets filtrados (cerca de 200.000). . Para volverlo un poco más divertido y patriota, utilizo un fondo con nuestra bandera pero se puede adaptar a cualquier tipo de imagen. . # Generación de un wordcloud paraguayo texto_tweets = &#39; &#39;.join(df_tweets_muni_filtro[&#39;tweet_cleaned&#39;]) mask = np.array(Image.open(&quot;../input/paraguay-flag/paraguay_flag_.png&quot;)) wordcloud_py = WordCloud(background_color=&quot;white&quot;, mode=&quot;RGBA&quot;, max_words=1000, mask=mask).generate(texto_tweets) # Utilización de colores de la imagen image_colors = ImageColorGenerator(mask) plt.figure(figsize=[12,8]) plt.imshow(wordcloud_py.recolor(color_func=image_colors), interpolation=&quot;bilinear&quot;) plt.axis(&quot;off&quot;) . (-0.5, 594.5, 223.5, -0.5) . Usuarios &#250;nicos&#182; . La cantidad de usuarios únicos que publican tweets nos puede ayudar a identificar situaciones que provocaron mayores picos de participación en el tiempo. Si nos fijamos la gráfica de abajo, se observan ciertos picos en el 2019, tanto en Mayo como en Julio. ¿Qué se mencionaba con frecuencia en estas fechas? . df_unique_users = df_tweets_muni_filtro.groupby([&#39;date&#39;])[&#39;username&#39;].nunique() . fig = plt.figure(figsize=(15,5)) sns.lineplot(x=&#39;date&#39;, y=&#39;username&#39;, data=df_unique_users.reset_index()) plt.title(&#39;Usuarios Unicos por Dia&#39;) plt.ylabel(&#39;Usuarios&#39;) plt.xlabel(&#39;Fecha&#39;) . /opt/conda/lib/python3.6/site-packages/pandas/plotting/_matplotlib/converter.py:103: FutureWarning: Using an implicitly registered datetime converter for a matplotlib plotting method. The converter was registered by pandas on import. Future versions of pandas will require you to explicitly register matplotlib converters. To register the converters: &gt;&gt;&gt; from pandas.plotting import register_matplotlib_converters &gt;&gt;&gt; register_matplotlib_converters() warnings.warn(msg, FutureWarning) . Text(0.5, 0, &#39;Fecha&#39;) . df_unique_users.reset_index().sort_values(&#39;username&#39;, ascending=False).head(5) . date username . 854 2019-05-11 | 1098 | . 853 2019-05-10 | 1083 | . 926 2019-07-23 | 973 | . 698 2018-12-03 | 861 | . 709 2018-12-14 | 782 | . &quot;gente puerca&quot;, &quot;basura calle&quot;, &quot;tira basura&quot;.. estos bigramas dan para pensar que en estas fechas pudo haber llovido con mucha frequencia, lo que pudo haber ocasionado raudales que a su vez movieron las basuras de un lado para otro. . fig = plt.figure(figsize=(10,5)) df_top_unique_days = df_tweets_muni_exploded[df_tweets_muni_exploded[&#39;date&#39;].isin([&#39;2019-05-10&#39;,&#39;2019-05-11&#39;])] .groupby(&#39;ngram&#39;) .count() .reset_index() .sort_values(&#39;date&#39;, ascending=False) .head(10) sns.barplot(x=&#39;date&#39;, y=&#39;ngram&#39;, data=df_top_unique_days, orient=&#39;h&#39;) plt.title(&#39;Top bigramas para 10-11/05/2019&#39;) plt.xlabel(&#39;Menciones&#39;) plt.ylabel(&#39;Bigrama&#39;) . Text(0, 0.5, &#39;Bigrama&#39;) . An&#225;lisis por eventos&#182; . Con lo analizado hasta ahora, tenemos una leve idea de los temas de conversación. Además, existen otros temas que pueden ser interesantes y no se encuentran a simple vista. Para entender un poco mejor como estos temas se mencionan en el tiempo, utilizo expresiones regulares que de acuerdo a ciertos patrones de búsqueda identifiquen tweets relacionados a los temas que nos interesan. En este caso, elegí los siguientes temas que me parecieron los más reclamados por la ciudadanía: basura, raudales, baches y dengue. Es importante tener en cuenta que un mismo tweet puede pertenecer a mas de una categoria porque de hecho puede existir correlacion entre varios de estos temas. . # Filtrar tweets por categorias de acuerdo a palabras claves def get_df_from_criteria(df, criteria): df = df[df[&#39;tweet_cleaned&#39;] .str .contains(criteria, flags=re.IGNORECASE, regex=True)] .groupby([&#39;date&#39;,&#39;year&#39;,&#39;month&#39;], as_index=False) .agg([&#39;count&#39;])[&#39;created_at&#39;].reset_index().rename(columns={&#39;count&#39;:&#39;tweets&#39;}) return df.copy() filtro_baches = r&#39; bbache| bvache| bcrater&#39; baches_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_baches) filtro_basura = r&#39; bbasura| brecicla| bdesecho| btoxico| bvertedero| bescombro| bsucio| basco&#39; basura_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_basura) filtro_inundado = r&#39; binunda| blluvia| bllueve| braudal| bdesagu*&#39; inundado_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_inundado) filtro_dengue = r&#39; bdengue| bmosquito| baedes| bcriadero| bminga| bfumiga&#39; dengue_x_dia_df = get_df_from_criteria(df_tweets_muni_filtro, filtro_dengue) . fig = plt.figure(figsize=(15,5)) ax = fig.add_subplot(111) sns.lineplot(x=&#39;date&#39;, y=&#39;tweets&#39;, data=baches_x_dia_df[baches_x_dia_df[&#39;year&#39;].isin([2017,2018,2019])], ax=ax) sns.lineplot(x=&#39;date&#39;, y=&#39;tweets&#39;, data=basura_x_dia_df[basura_x_dia_df[&#39;year&#39;].isin([2017,2018,2019])], ax=ax) sns.lineplot(x=&#39;date&#39;, y=&#39;tweets&#39;, data=inundado_x_dia_df[inundado_x_dia_df[&#39;year&#39;].isin([2017,2018,2019])], ax=ax) sns.lineplot(x=&#39;date&#39;, y=&#39;tweets&#39;, data=dengue_x_dia_df[dengue_x_dia_df[&#39;year&#39;].isin([2017,2018,2019])], ax=ax) plt.title(&#39;Cantidad de Menciones por Dia&#39;) plt.ylabel(&#39;Menciones&#39;) plt.xlabel(&#39;Fecha&#39;) ax.legend([&#39;Baches&#39;, &#39;Basura&#39;, &#39;Raudales&#39;, &#39;Dengue&#39;], loc=&#39;upper left&#39;, prop={&#39;size&#39;: 12}) . &lt;matplotlib.legend.Legend at 0x7f9491c8c828&gt; . En Mayo vemos picos de menciones a raudales que coinciden con los picos de usuarios unicos que vimos mas arriba. También se observa un pico de conversaciones relacionadas al dengue en Marzo de 2018 que coincide con uno de los brotes mas importantes en los últimos años. . Para facilitar comparaciones interanuales, en las próximas gráficas podemos ver para cada tema como cambian las menciones en el tiempo. En el caso de dengue por ejemplo, se observa un aumento importante en Marzo de 2018. Por otro lado, en las menciones de raudales se ve un crecimiento notorio en Mayo de 2019 en comparación con años anteriores. . # Agrupar por mes dengue_x_mes_df = dengue_x_dia_df.groupby([&#39;year&#39;,&#39;month&#39;]) .sum().reset_index()[[&#39;month&#39;,&#39;year&#39;,&#39;tweets&#39;]] # Pivotear dengue_x_mes_pivot = dengue_x_mes_df .pivot(index=&#39;month&#39;,columns=&#39;year&#39;, values=&#39;tweets&#39;) dengue_x_mes_pivot.plot(kind=&#39;bar&#39;, figsize=(15, 5), color=[&#39;lightgray&#39;, &#39;gray&#39;, &#39;black&#39;], rot=0) plt.title(&quot;Comparaciones interanuales de menciones de Dengue&quot;) plt.xlabel(&quot;Mes&quot;, labelpad=16) plt.ylabel(&quot;Menciones (Dengue)&quot;, labelpad=16) . Text(0, 0.5, &#39;Menciones (Dengue)&#39;) . # Agrupar por mes basura_x_mes_df = basura_x_dia_df.groupby([&#39;year&#39;,&#39;month&#39;]) .sum().reset_index()[[&#39;month&#39;,&#39;year&#39;,&#39;tweets&#39;]] # Pivotear basura_x_mes_pivot = basura_x_mes_df .pivot(index=&#39;month&#39;,columns=&#39;year&#39;, values=&#39;tweets&#39;) basura_x_mes_pivot.plot(kind=&#39;bar&#39;, figsize=(15, 5), color=[&#39;lightgray&#39;, &#39;gray&#39;, &#39;black&#39;], rot=0) plt.title(&quot;Comparaciones interanuales de menciones de Basura&quot;) plt.xlabel(&quot;Mes&quot;, labelpad=16) plt.ylabel(&quot;Menciones (Basura)&quot;, labelpad=16) . Text(0, 0.5, &#39;Menciones (Basura)&#39;) . # Agrupar por mes inundado_x_mes_df = inundado_x_dia_df.groupby([&#39;year&#39;,&#39;month&#39;]) .sum().reset_index()[[&#39;month&#39;,&#39;year&#39;,&#39;tweets&#39;]] # Pivotear inundado_x_mes_pivot = inundado_x_mes_df .pivot(index=&#39;month&#39;,columns=&#39;year&#39;, values=&#39;tweets&#39;) inundado_x_mes_pivot.plot(kind=&#39;bar&#39;, figsize=(15, 5), color=[&#39;lightgray&#39;, &#39;gray&#39;, &#39;black&#39;], rot=0) plt.title(&quot;Comparaciones interanuales de menciones de Raudales&quot;) plt.xlabel(&quot;Mes&quot;, labelpad=16) plt.ylabel(&quot;Menciones (Raudales)&quot;, labelpad=16) . Text(0, 0.5, &#39;Menciones (Raudales)&#39;) . Modelado de Topicos con LDA&#182; . Para casos como este, dónde tenemos una buena idea de los temas de conversación en los tweets, las expresiones regulares pueden ser suficientes. Sin embargo, existen otros casos dónde se necesitan de técnicas más avanzadas para identificar temas que pueden estar escondidos, o latentes. . Existen diferentes técnicas de identificación de temas o tópicos pero una de las más utilizadas es Latent Dirichlet Allocation o LDA. Se trata de una técnica que genera un modelo probabilístico que asume que cada tema es una combinación de palabras y que cada documento (o tweet en este caso) es una combinación de temas con diferentes probabilidades. . En las celdas de abajo, creo un Pipeline que primero aplica una técnica conocida como TF-IDF que calcula la frecuencia de palabras en los tweets, y calcula un score para cada palabra dando menos importancia a aquellas que aparecen con demasiada frecuencia y son poco relevantes. En el siguiente paso del pipeline se entrena el modelo y transforma el dataframe original. En este caso, elijo clasificar los temas en 5 diferentes categorias pero el número puede variar de acuerdo al caso. Existen formas de medir la calidad del modelo, calculando lo que se denomina perplexity pero no lo voy a utilizar en este caso. . Otro punto importante es que el entrenamiento del modelo puede tomar mucho tiempo y por esto es clave definir ciertos parametros que limiten la busqueda y la cantidad de iteraciones. . # Cantidad de topicos n_topics = 5 # Pipeline con pasos a ejectuar text_pipeline = Pipeline([ (&#39;tfidf&#39;, TfidfVectorizer(ngram_range=(2,3), min_df=100, max_df=0.85)), (&#39;lda&#39;, LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method=&#39;online&#39;)) ]) t0 = time() # Entrenar y transformar modelo lda_model = text_pipeline.fit_transform(df_tweets_muni_filtro[&#39;tweet_cleaned&#39;]) print(time() - t0) . 129.6550097465515 . # Modelos resultantes del pipeline tfidf = text_pipeline.steps[0][1] lda = text_pipeline.steps[1][1] vocabulario = tfidf.get_feature_names() . top_topics = 5 topic_dict = {} topic_scores = [] # Para cada topico, buscamos el top 5 de acuerdo a los scores calculados por el modelo for topic_idx, topic in enumerate(lda.components_): topic_dict[str(topic_idx)] = &quot;,&quot;.join([vocabulario[i] for i in topic.argsort()[:-top_topics - 1:-1]]) topic_scores.append([topic[i] for i in topic.argsort()[:-top_topics - 1:-1]]) df_topics_lda = pd.DataFrame(topic_dict, index=[&#39;bigrams&#39;]) df_topics_lda = df_topics_lda.T.reset_index() # Cargamos las palabras claves por topico en un dataframe df_topics_names = pd.DataFrame(df_topics_lda.bigrams.str.split(&#39;,&#39;).tolist(), index=df_topics_lda.index) .stack() .reset_index() .drop([&#39;level_1&#39;], axis=1) .rename(columns={0:&#39;bigrams&#39;, &#39;level_0&#39;:&#39;topic&#39;}) # Cargamos scores por topicos en un dataframe df_topics_scores = pd.DataFrame(topic_scores) .stack() .reset_index(drop=True) # Concatenamos palabras claves con scores correspondientes df_topics = pd.concat([df_topics_names, df_topics_scores], axis=1) .rename(columns={0:&#39;score&#39;}) . Finalmente se observan los bigramas mas frecuentes para cada una de las categorias. No se identifican categorias especificas y esto puede deberse a que no existe una diferenciacion muy clara entre los temas de conversacion. También se pueden obtener mejores resultados realizando validaciones adicionales y excluyendo otras combinaciones poco relevantes. . fig = plt.figure(figsize=(12,10)) for i in range(1,n_topics+1): plt.subplot(3,2,i,frameon=True) sns.barplot(&#39;score&#39;, &#39;bigrams&#39;, data=df_topics[df_topics[&#39;topic&#39;]==i-1], orient=&#39;h&#39;) plt.title(&quot;Topico {}&quot;.format(i)) plt.xlabel(&#39;&#39;) plt.ylabel(&#39;&#39;) plt.tight_layout() . Otros an&#225;lisis que se podrian realizar con estos datos:&#182; . Análisis de sentimiento | Correlación entre temas y visualización con scattertext | Series temporales para predecir eventos (como brotes de dengue) | .",
            "url": "https://www.dataguasu.com/blog/twitter/nlp/analisis/2019/11/18/analisis_tweets_municipalidad.html",
            "relUrl": "/twitter/nlp/analisis/2019/11/18/analisis_tweets_municipalidad.html",
            "date": " • Nov 18, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://www.dataguasu.com/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  
  

  

  
  

  

  
      ,"page8": {
          "title": "",
          "content": "Warning . Do not manually save images into this folder. This is used by GitHub Actions to automatically copy images. Any images you save into this folder could be deleted at build time. .",
          "url": "https://www.dataguasu.com/blog/images/copied_from_nb/",
          "relUrl": "/images/copied_from_nb/",
          "date": ""
      }
      
  

  
  

}